{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram Multi-User Scraper (DOM-based, Mobile View)\n",
    "Item-by-item downloading with smart scroll estimation\n",
    "Downloads ALL valid images/videos at each carousel position (URL-tracked globally to avoid duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USERNAME = \"david_galvinn\"  # Your Instagram username\n",
    "PASSWORD = \"wx48694062\"  # Your Instagram password\n",
    "\n",
    "# List of usernames to scrape\n",
    "# USERS_TO_SCRAPE = [\"iampatcortez\"]  # e.g., [\"user1\", \"user2\", \"user3\"]\n",
    "USERS_TO_SCRAPE = [\"_alexandramarasigan\", \n",
    "                   \"aeropalmics\", \"afffirmations\", \"agatapanucci\", \"akshii09\", \"anaasotillo\", \"antonianocca\", \n",
    "                   \"artbutmakeitsports\", \n",
    "                   \"badboyshah\", \"beamarinx\", \"beateliljegren\", \"benne_nc\", \"brettsbites\", \"brianaking\", \n",
    "                   \"britishvogue\", \"broadcastboyshoh\", \n",
    "                   \"chargers\", \"chezkacarandang\", \"cle_lya\", \n",
    "                   \"dagna.kills\", \"dsarm\", \n",
    "                   \"elbavdh\", \"elenislafuente\", \"elinevanhaasteren\", \n",
    "                   \"goalusa_\", \n",
    "                   \"itsantonia\", \"indivisa\", \n",
    "                   \"jacopodecarli\", \"johannpillas\", \"jules_duthu\", \"juliahobbs_\", \"justshowupclub\", \"jkrewfam\", \n",
    "                   \"jpbdeleon\", \n",
    "                   \"kgunaa\", \"kickiyangz\", \"kontorns\", \"kattvaldez\", \n",
    "                   \"lafamilycurly\", \"lisa.zimouche\", \n",
    "                   \"mariecooles\", \"marrshoe\", \"mickey.a.np\", \"mishtikhatri\", \n",
    "                   \"nolendubuc\", \n",
    "                   \"overtimewbb\", \n",
    "                   \"poojadhingra\", \"pgatour\", \n",
    "                   \"raahavy_\", \"rapruesign\", \"riamegan\", \"rubinpatrick_\", \"runyoko\", \"redcarpetgirlz\", \n",
    "                   \"sausagelord\", \"shreyasiyer96\", \"silvypavida\", \"superbabekillah\", \"sunday.scaries\", \n",
    "                   \"thekabsfamily\", \"thatsamazing\", \n",
    "                   \"victoriabennie\", \n",
    "                   \"wenevergetyounger\"]\n",
    "# USERS_TO_SCRAPE = []\n",
    "USERS_TO_SCRAPE = [\"brianaking\"]\n",
    "\n",
    "# Cutoff date - only scrape posts from this date forward\n",
    "CUTOFF_DATE = datetime(2023, 8, 17, 0, 0, 0)\n",
    "\n",
    "# Sleep multiplier - set to 2 or 3 to slow down (default 1)\n",
    "SLEEP_MULTIPLIER = 2  # Change to 2 or 3 if you need slower execution\n",
    "\n",
    "BASE_DOWNLOAD_DIR = Path(\"instagram_downloads\")\n",
    "BASE_DOWNLOAD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Global tracking\n",
    "downloaded_hashes = set()\n",
    "seen_urls = set()  # Track image URLs globally to avoid re-downloading\n",
    "stats_log = []\n",
    "\n",
    "# Logging setup\n",
    "log_lines = []\n",
    "original_print = print\n",
    "\n",
    "def custom_print(*args, **kwargs):\n",
    "    # Capture the message\n",
    "    message = ' '.join(str(arg) for arg in args)\n",
    "    log_lines.append(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
    "    # Call original print\n",
    "    original_print(*args, **kwargs)\n",
    "\n",
    "print = custom_print\n",
    "\n",
    "# Current user being scraped (set by scrape_user)\n",
    "CURRENT_USER = None\n",
    "DOWNLOAD_DIR = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Browser opened with mobile emulation\n",
      "Sleep multiplier: 2x\n"
     ]
    }
   ],
   "source": [
    "# Setup Chrome with mobile emulation\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('mobileEmulation', {\n",
    "    'deviceName': 'iPhone 12 Pro'\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "print(\"✓ Browser opened with mobile emulation\")\n",
    "print(f\"Sleep multiplier: {SLEEP_MULTIPLIER}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged in successfully\n"
     ]
    }
   ],
   "source": [
    "# Login to Instagram\n",
    "def login_instagram(username, password):\n",
    "    driver.get('https://www.instagram.com/')\n",
    "    time.sleep(6 * SLEEP_MULTIPLIER)\n",
    "    \n",
    "    try:\n",
    "        # Wait for and click login button if on homepage\n",
    "        try:\n",
    "            login_link = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//a[contains(@href, '/accounts/login')]\"))\n",
    "            )\n",
    "            login_link.click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Enter username\n",
    "        username_input = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"username\"))\n",
    "        )\n",
    "        username_input.send_keys(username)\n",
    "        \n",
    "        # Enter password\n",
    "        password_input = driver.find_element(By.NAME, \"password\")\n",
    "        password_input.send_keys(password)\n",
    "        password_input.send_keys(Keys.RETURN)\n",
    "                \n",
    "        # Handle \"Save Your Login Info\" popup\n",
    "        try:\n",
    "            not_now = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not now') or contains(text(), 'Not Now')]\"))\n",
    "            )\n",
    "            not_now.click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Handle \"Turn on Notifications\" popup\n",
    "        try:\n",
    "            not_now = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not Now')]\"))\n",
    "            )\n",
    "            not_now.click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(\"✓ Logged in successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Perform login\n",
    "if USERNAME and PASSWORD:\n",
    "    login_instagram(USERNAME, PASSWORD)\n",
    "else:\n",
    "    print(\"⚠ No login credentials provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract post date from a post page\n",
    "def extract_post_date():\n",
    "    try:\n",
    "        time_elements = driver.find_elements(By.CSS_SELECTOR, 'time.x1p4m5qa')\n",
    "        if time_elements:\n",
    "            datetime_str = time_elements[0].get_attribute('datetime')\n",
    "            if datetime_str:\n",
    "                post_date = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))\n",
    "                return post_date.replace(tzinfo=None)\n",
    "    except:\n",
    "        pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract post caption/text\n",
    "def extract_post_caption():\n",
    "    try:\n",
    "        # Look for h1 with caption text\n",
    "        caption_elements = driver.find_elements(By.CSS_SELECTOR, 'h1._ap3a._aaco._aacu._aacx._aad7._aade')\n",
    "        if caption_elements:\n",
    "            return caption_elements[0].text\n",
    "    except:\n",
    "        pass\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (sample_post_dates function removed - no longer needed)\n",
    "def estimate_scrolls_needed(username):\n",
    "    return 18  # Temporary fixed return for testing # 361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Estimate scrolls needed to reach cutoff date\n",
    "# def estimate_scrolls_needed(username):\n",
    "#     return 7  # Temporary fixed return for testing\n",
    "#     print(\"\\nEstimating scrolls needed...\")\n",
    "    \n",
    "#     profile_url = f\"https://www.instagram.com/{username}/\"\n",
    "#     driver.get(profile_url)\n",
    "#     time.sleep(6 * SLEEP_MULTIPLIER)\n",
    "    \n",
    "#     # Scroll 1 time and collect links\n",
    "#     all_links_1_scroll = []\n",
    "#     driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#     time.sleep(4 * SLEEP_MULTIPLIER)\n",
    "    \n",
    "#     elements = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"/p/\"], a[href*=\"/reel/\"]')\n",
    "#     seen = set()\n",
    "#     for elem in elements:\n",
    "#         href = elem.get_attribute('href')\n",
    "#         if href and ('/p/' in href or '/reel/' in href) and href not in seen:\n",
    "#             all_links_1_scroll.append(href)\n",
    "#             seen.add(href)\n",
    "    \n",
    "#     print(f\"After 1 scroll: {len(all_links_1_scroll)} posts collected\")\n",
    "    \n",
    "#     if not all_links_1_scroll:\n",
    "#         print(\"No posts found, using default scroll count\")\n",
    "#         return 50\n",
    "    \n",
    "#     # Get date from last post (oldest visible)\n",
    "#     print(\"Checking date of last post after 1 scroll...\")\n",
    "#     driver.get(all_links_1_scroll[-1])\n",
    "#     time.sleep(3 * SLEEP_MULTIPLIER)\n",
    "#     earliest_1_scroll = extract_post_date()\n",
    "    \n",
    "#     if not earliest_1_scroll:\n",
    "#         print(\"Could not extract date, using default scroll count\")\n",
    "#         return 50\n",
    "    \n",
    "#     print(f\"Earliest date after 1 scroll: {earliest_1_scroll.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "#     # Go back and scroll 3 times, collecting links after each scroll\n",
    "#     driver.get(profile_url)\n",
    "#     time.sleep(6 * SLEEP_MULTIPLIER)\n",
    "    \n",
    "#     all_links_3_scroll = []\n",
    "#     seen = set()\n",
    "#     for i in range(3):\n",
    "#         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#         time.sleep(4 * SLEEP_MULTIPLIER)\n",
    "        \n",
    "#         # Collect links after each scroll\n",
    "#         elements = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"/p/\"], a[href*=\"/reel/\"]')\n",
    "#         for elem in elements:\n",
    "#             href = elem.get_attribute('href')\n",
    "#             if href and ('/p/' in href or '/reel/' in href) and href not in seen:\n",
    "#                 all_links_3_scroll.append(href)\n",
    "#                 seen.add(href)\n",
    "    \n",
    "#     print(f\"After 3 scrolls: {len(all_links_3_scroll)} posts collected\")\n",
    "    \n",
    "#     if not all_links_3_scroll:\n",
    "#         print(\"No posts found, using default scroll count\")\n",
    "#         return 50\n",
    "    \n",
    "#     # Get date from last post (oldest visible)\n",
    "#     print(\"Checking date of last post after 3 scrolls...\")\n",
    "#     driver.get(all_links_3_scroll[-1])\n",
    "#     time.sleep(3 * SLEEP_MULTIPLIER)\n",
    "#     earliest_3_scroll = extract_post_date()\n",
    "    \n",
    "#     if not earliest_3_scroll:\n",
    "#         print(\"Could not extract date, using default scroll count\")\n",
    "#         return 50\n",
    "    \n",
    "#     print(f\"Earliest date after 3 scrolls: {earliest_3_scroll.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "#     time_diff = earliest_1_scroll - earliest_3_scroll\n",
    "#     days_per_2_scrolls = time_diff.days\n",
    "    \n",
    "#     if days_per_2_scrolls <= 0:\n",
    "#         print(\"Time difference too small, using default scroll count\")\n",
    "#         return 50\n",
    "    \n",
    "#     print(f\"Time covered by 2 scrolls: {days_per_2_scrolls} days\")\n",
    "    \n",
    "#     days_to_cutoff = (earliest_3_scroll - CUTOFF_DATE).days\n",
    "    \n",
    "#     if days_to_cutoff <= 0:\n",
    "#         print(f\"Already reached cutoff date!\")\n",
    "#         return 3\n",
    "    \n",
    "#     additional_scrolls = int((days_to_cutoff / days_per_2_scrolls) * 2) + 5\n",
    "#     total_scrolls = 3 + additional_scrolls\n",
    "    \n",
    "#     print(f\"Days to cutoff: {days_to_cutoff}\")\n",
    "#     print(f\"Estimated total scrolls needed: {total_scrolls}\")\n",
    "    \n",
    "#     if total_scrolls > 200:\n",
    "#         print(\"Capping at 200 scrolls\")\n",
    "#         return 200\n",
    "    \n",
    "#     return total_scrolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract post stats\n",
    "def extract_post_stats():\n",
    "    likes = \"0\"\n",
    "    comments = \"0\"\n",
    "    is_paid = False\n",
    "    \n",
    "    try:\n",
    "        like_spans = driver.find_elements(By.CSS_SELECTOR, 'span.x1ypdohk.x1s688f.x2fvf9.xe9ewy2[role=\"button\"]')\n",
    "        if like_spans:\n",
    "            likes = like_spans[0].text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        comment_spans = driver.find_elements(\n",
    "            By.CSS_SELECTOR,\n",
    "            'span.xdj266r.x14z9mp.xat24cr.x1lziwak.xexx8yu.xyri2b.x18d9i69.x1c1uobl.x1hl2dhg.x16tdsg8.x1vvkbs'\n",
    "        )\n",
    "        for span in comment_spans:\n",
    "            text = span.text\n",
    "            if text.replace(',', '').isdigit():\n",
    "                comments = text\n",
    "                break\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if \"Paid partnership\" in driver.page_source:\n",
    "            is_paid = True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return likes, comments, is_paid\n",
    "\n",
    "# Extract posting account (subbrand)\n",
    "def extract_posting_account():\n",
    "    try:\n",
    "        # Look for the posting account span (e.g., nikerunning as subbrand of nike)\n",
    "        account_spans = driver.find_elements(\n",
    "            By.CSS_SELECTOR,\n",
    "            'span.x193iq5w.xeuugli.x1fj9vlw.x13faqbe.x1vvkbs.xt0psk2.x1i0vuye.xvs91rp.x1s688f.x5n08af.x10wh9bi.xpm28yp.x8viiok.x1o7cslx'\n",
    "        )\n",
    "        if account_spans:\n",
    "            # Return the first one that looks like a username\n",
    "            for span in account_spans:\n",
    "                text = span.text.strip()\n",
    "                if text and len(text) > 0 and not text.isdigit():\n",
    "                    return text\n",
    "    except:\n",
    "        pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract current carousel item media URLs (returns ALL valid images/videos)\n",
    "def extract_current_item_urls():\n",
    "    media_urls = []\n",
    "    \n",
    "    try:\n",
    "        # Check for video first (priority)\n",
    "        videos = driver.find_elements(By.TAG_NAME, 'video')\n",
    "        for video in videos:\n",
    "            src = video.get_attribute('src')\n",
    "            if src and ('cdninstagram.com' in src or 'fbcdn.net' in src):\n",
    "                if src not in seen_urls:\n",
    "                    media_urls.append((src, 'video'))\n",
    "        \n",
    "        # Get ALL valid images (not just first one)\n",
    "        images = driver.find_elements(By.TAG_NAME, 'img')\n",
    "        \n",
    "        for img in images:\n",
    "            src = img.get_attribute('src')\n",
    "            if src and ('cdninstagram.com' in src or 'fbcdn.net' in src):\n",
    "                # Filter out unwanted images\n",
    "                if any(x in src for x in ['/s150x150/', '/s320x320/', 's640x640', 'static']):\n",
    "                    continue\n",
    "                \n",
    "                # Filter out 150x150 profile pics by dimension\n",
    "                try:\n",
    "                    width = img.get_attribute('width')\n",
    "                    height = img.get_attribute('height')\n",
    "                    if width and height:\n",
    "                        if int(width) == 150 and int(height) == 150:\n",
    "                            continue\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Skip if we've already seen this URL\n",
    "                if src in seen_urls:\n",
    "                    continue\n",
    "                \n",
    "                # Add ALL valid images (better to have more than miss content)\n",
    "                media_urls.append((src, 'image'))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error extracting media: {e}\")\n",
    "    \n",
    "    return media_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if next button exists\n",
    "def has_next_button():\n",
    "    try:\n",
    "        selectors = [\n",
    "            'button[aria-label=\"Next\"]',\n",
    "            'button[aria-label=\"next\"]',\n",
    "            'button._afxw._al46._al47'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                next_btn = WebDriverWait(driver, 2).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, selector))\n",
    "                )\n",
    "                return next_btn\n",
    "            except:\n",
    "                continue\n",
    "        return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download single file\n",
    "def download_file(url, filepath):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check for duplicates\n",
    "        content_hash = hashlib.md5(response.content).hexdigest()\n",
    "        if content_hash in downloaded_hashes:\n",
    "            return False, \"duplicate\"\n",
    "        \n",
    "        downloaded_hashes.add(content_hash)\n",
    "        filepath.write_bytes(response.content)\n",
    "        return True, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, str(e)[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape a single post (item-by-item download, all images per position)\n",
    "def scrape_post(post_url):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping: {post_url}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    driver.get(post_url)\n",
    "    time.sleep(6 * SLEEP_MULTIPLIER)\n",
    "    \n",
    "    # Extract date\n",
    "    post_date = extract_post_date()\n",
    "    if post_date:\n",
    "        print(f\"Date: {post_date.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Extract stats\n",
    "    likes, comments, is_paid = extract_post_stats()\n",
    "    \n",
    "    # Extract posting account (subbrand)\n",
    "    posting_account = extract_posting_account()\n",
    "    if posting_account:\n",
    "        print(f\"Posted by: {posting_account}\")\n",
    "    \n",
    "    print(f\"Stats: Likes={likes}, Comments={comments}, Paid={is_paid}\")\n",
    "    \n",
    "    # Extract caption\n",
    "    caption = extract_post_caption()\n",
    "    \n",
    "    # Create directory for this post\n",
    "    post_id = post_url.rstrip('/').split('/')[-1]\n",
    "    post_dir = DOWNLOAD_DIR / post_id\n",
    "    post_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save caption\n",
    "    if caption:\n",
    "        caption_file = post_dir / \"caption.txt\"\n",
    "        caption_file.write_text(caption, encoding='utf-8')\n",
    "        print(f\"Saved caption ({len(caption)} chars)\")\n",
    "    \n",
    "    # Download carousel items one by one\n",
    "    carousel_position = 1\n",
    "    total_items_downloaded = 0\n",
    "    \n",
    "    while True:\n",
    "        # Extract ALL media URLs at current position\n",
    "        media_urls = extract_current_item_urls()\n",
    "        \n",
    "        if media_urls:\n",
    "            print(f\"\\n  Carousel position {carousel_position}: Found {len(media_urls)} items\")\n",
    "            \n",
    "            # Download each media item\n",
    "            for url, media_type in media_urls:\n",
    "                ext = '.mp4' if media_type == 'video' else '.jpg'\n",
    "                \n",
    "                # Find next available filename\n",
    "                item_num = 1\n",
    "                while True:\n",
    "                    filepath = post_dir / f\"item_{item_num}{ext}\"\n",
    "                    if not filepath.exists():\n",
    "                        break\n",
    "                    item_num += 1\n",
    "                \n",
    "                success, error = download_file(url, filepath)\n",
    "                if success:\n",
    "                    print(f\"    Downloaded item_{item_num}{ext} ({media_type})\")\n",
    "                    seen_urls.add(url)  # Mark URL as seen\n",
    "                    total_items_downloaded += 1\n",
    "                else:\n",
    "                    if error == \"duplicate\":\n",
    "                        print(f\"    Skipped item_{item_num}: Duplicate content\")\n",
    "                        seen_urls.add(url)  # Still mark as seen\n",
    "                    else:\n",
    "                        print(f\"    Failed item_{item_num}: {error}\")\n",
    "                \n",
    "                time.sleep(5 * SLEEP_MULTIPLIER)\n",
    "        \n",
    "        # Try to click next\n",
    "        next_btn = has_next_button()\n",
    "        if not next_btn:\n",
    "            break\n",
    "        \n",
    "        next_btn.click()\n",
    "        time.sleep(1 * SLEEP_MULTIPLIER)\n",
    "        carousel_position += 1\n",
    "    \n",
    "    print(f\"\\nProcessed {carousel_position} carousel positions\")\n",
    "    \n",
    "    # Log stats\n",
    "    stats_log.append({\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'post_url': post_url,\n",
    "        'post_id': post_id,\n",
    "        'post_date': post_date.isoformat() if post_date else None,\n",
    "        'posting_account': posting_account,\n",
    "        'likes': likes,\n",
    "        'comments': comments,\n",
    "        'paid_partnership': is_paid,\n",
    "        'caption_length': len(caption) if caption else 0,\n",
    "        'carousel_positions': carousel_position,\n",
    "        'media_downloaded': total_items_downloaded\n",
    "    })\n",
    "    \n",
    "    print(f\"✓ Downloaded {total_items_downloaded} unique items to '{post_dir}'\")\n",
    "    return total_items_downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape user profile\n",
    "def scrape_user(username, max_posts=None):\n",
    "    global CURRENT_USER, DOWNLOAD_DIR, stats_log, log_lines\n",
    "    \n",
    "    # Set up user-specific directory\n",
    "    CURRENT_USER = username\n",
    "    DOWNLOAD_DIR = BASE_DOWNLOAD_DIR / username\n",
    "    DOWNLOAD_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load existing stats and logs if they exist (for resuming)\n",
    "    stats_file = DOWNLOAD_DIR / \"scrape_stats.json\"\n",
    "    if stats_file.exists():\n",
    "        with open(stats_file, 'r') as f:\n",
    "            stats_log = json.load(f)\n",
    "        print(f\"Loaded {len(stats_log)} existing stats entries\")\n",
    "    else:\n",
    "        stats_log = []\n",
    "    \n",
    "    log_file = DOWNLOAD_DIR / \"log.txt\"\n",
    "    if log_file.exists():\n",
    "        with open(log_file, 'r', encoding='utf-8') as f:\n",
    "            log_lines = f.read().splitlines()\n",
    "        print(f\"Loaded {len(log_lines)} existing log lines\")\n",
    "    else:\n",
    "        log_lines = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping user: {username}\")\n",
    "    print(f\"Cutoff date: {CUTOFF_DATE.strftime('%Y-%m-%d')}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Estimate scrolls needed\n",
    "    estimated_scrolls = estimate_scrolls_needed(username)\n",
    "    \n",
    "    # Scroll profile and collect links incrementally\n",
    "    print(f\"\\nScrolling {estimated_scrolls} times...\")\n",
    "    profile_url = f\"https://www.instagram.com/{username}/\"\n",
    "    driver.get(profile_url)\n",
    "    time.sleep(6 * SLEEP_MULTIPLIER)\n",
    "    \n",
    "    all_post_links = set()\n",
    "    \n",
    "    for i in range(estimated_scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(4 * SLEEP_MULTIPLIER)\n",
    "        \n",
    "        # Collect links after each scroll\n",
    "        links = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"/p/\"], a[href*=\"/reel/\"]')\n",
    "        for link in links:\n",
    "            href = link.get_attribute('href')\n",
    "            if href and ('/p/' in href or '/reel/' in href):\n",
    "                all_post_links.add(href)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Scrolled {i + 1}/{estimated_scrolls} times... ({len(all_post_links)} posts collected so far)\")\n",
    "    \n",
    "    print(f\"Completed {estimated_scrolls} scrolls\")\n",
    "    \n",
    "    # Convert set to list\n",
    "    post_links = list(all_post_links)\n",
    "    \n",
    "    if max_posts:\n",
    "        post_links = post_links[:max_posts]\n",
    "    \n",
    "    print(f\"Found {len(post_links)} unique posts (including reels)\")\n",
    "    \n",
    "    # Scrape each post\n",
    "    try:\n",
    "        total_downloaded = 0\n",
    "        skipped_old = 0\n",
    "        skipped_existing = 0\n",
    "        \n",
    "        for i, post_url in enumerate(post_links, 1):\n",
    "            print(f\"\\nPost {i}/{len(post_links)}\")\n",
    "\n",
    "            # Extract post ID and check if folder exists\n",
    "            post_id = post_url.rstrip('/').split('/')[-1]\n",
    "            post_dir = DOWNLOAD_DIR / post_id\n",
    "            \n",
    "            if post_dir.exists() and any(post_dir.iterdir()):\n",
    "                print(f\"⏩ Skipping post {post_id} (folder already exists)\")\n",
    "                skipped_existing += 1\n",
    "                continue\n",
    "            \n",
    "            # Check post date before downloading\n",
    "            driver.get(post_url)\n",
    "            time.sleep(3 * SLEEP_MULTIPLIER)\n",
    "            \n",
    "            post_date = extract_post_date()\n",
    "            if post_date and post_date < CUTOFF_DATE:\n",
    "                print(f\"⏩ Skipping post from {post_date.strftime('%Y-%m-%d')} (before cutoff)\")\n",
    "                skipped_old += 1\n",
    "                continue\n",
    "            \n",
    "            # Go back to post URL to scrape it properly\n",
    "            count = scrape_post(post_url)\n",
    "            total_downloaded += count\n",
    "            time.sleep(4 * SLEEP_MULTIPLIER)\n",
    "        \n",
    "        print(f\"\\n✓ User '{username}' complete:\")\n",
    "        print(f\"  Items downloaded: {total_downloaded}\")\n",
    "        print(f\"  Posts skipped (already exists): {skipped_existing}\")\n",
    "        print(f\"  Posts skipped (before cutoff): {skipped_old}\")\n",
    "        \n",
    "        # Save stats log for this user\n",
    "        if stats_log:\n",
    "            stats_file = DOWNLOAD_DIR / \"scrape_stats.json\"\n",
    "            with open(stats_file, 'w') as f:\n",
    "                json.dump(stats_log, f, indent=2)\n",
    "            print(f\"\\n✓ Stats saved to {stats_file}\")\n",
    "            print(f\"\\nTotal stats:\")\n",
    "            print(f\"  Posts scraped: {len(stats_log)}\")\n",
    "            print(f\"  Items downloaded: {sum(s['media_downloaded'] for s in stats_log)}\")\n",
    "            print(f\"  Paid partnerships: {sum(1 for s in stats_log if s['paid_partnership'])}\")\n",
    "        \n",
    "        # Save log file for this user\n",
    "        if log_lines:\n",
    "            log_file = DOWNLOAD_DIR / \"log.txt\"\n",
    "            with open(log_file, 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(log_lines))\n",
    "            print(f\"✓ Log saved to {log_file}\")\n",
    "        \n",
    "        return total_downloaded\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping user: {e}\")\n",
    "        \n",
    "        # Save stats and log even on error\n",
    "        if stats_log:\n",
    "            stats_file = DOWNLOAD_DIR / \"scrape_stats.json\"\n",
    "            with open(stats_file, 'w') as f:\n",
    "                json.dump(stats_log, f, indent=2)\n",
    "        \n",
    "        if log_lines:\n",
    "            log_file = DOWNLOAD_DIR / \"log.txt\"\n",
    "            with open(log_file, 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(log_lines))\n",
    "        \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Scraping user: brianaking\n",
      "Cutoff date: 2023-08-17\n",
      "============================================================\n",
      "\n",
      "Scrolling 18 times...\n",
      "  Scrolled 10/18 times... (144 posts collected so far)\n",
      "Completed 18 scrolls\n",
      "Found 240 unique posts (including reels)\n",
      "\n",
      "Post 1/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/p/DC1tCFlvTzK/\n",
      "============================================================\n",
      "Date: 2024-11-26 15:10:44\n",
      "Posted by: brianaking\n",
      "Stats: Likes=1.3K, Comments=37, Paid=False\n",
      "Saved caption (92 chars)\n",
      "\n",
      "  Carousel position 1: Found 4 items\n",
      "    Downloaded item_1.jpg (image)\n",
      "    Downloaded item_2.jpg (image)\n",
      "    Downloaded item_3.jpg (image)\n",
      "    Downloaded item_4.jpg (image)\n",
      "\n",
      "  Carousel position 3: Found 1 items\n",
      "    Downloaded item_5.jpg (image)\n",
      "\n",
      "  Carousel position 4: Found 1 items\n",
      "    Downloaded item_6.jpg (image)\n",
      "\n",
      "  Carousel position 5: Found 1 items\n",
      "    Downloaded item_7.jpg (image)\n",
      "\n",
      "  Carousel position 6: Found 1 items\n",
      "    Downloaded item_8.jpg (image)\n",
      "\n",
      "  Carousel position 7: Found 1 items\n",
      "    Downloaded item_9.jpg (image)\n",
      "\n",
      "  Carousel position 8: Found 1 items\n",
      "    Downloaded item_10.jpg (image)\n",
      "\n",
      "  Carousel position 9: Found 1 items\n",
      "    Downloaded item_11.jpg (image)\n",
      "\n",
      "  Carousel position 10: Found 1 items\n",
      "    Downloaded item_12.jpg (image)\n",
      "\n",
      "Processed 11 carousel positions\n",
      "✓ Downloaded 12 unique items to 'instagram_downloads\\brianaking\\DC1tCFlvTzK'\n",
      "\n",
      "Post 2/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/p/DHG3t4VvUSu/\n",
      "============================================================\n",
      "Date: 2025-03-12 17:16:45\n",
      "Posted by: brianaking\n",
      "Stats: Likes=1K, Comments=40, Paid=False\n",
      "Saved caption (112 chars)\n",
      "\n",
      "  Carousel position 1: Found 4 items\n",
      "    Skipped item_1: Duplicate content\n",
      "    Downloaded item_1.jpg (image)\n",
      "    Downloaded item_2.jpg (image)\n",
      "    Downloaded item_3.jpg (image)\n",
      "\n",
      "  Carousel position 3: Found 1 items\n",
      "    Downloaded item_4.jpg (image)\n",
      "\n",
      "  Carousel position 4: Found 1 items\n",
      "    Downloaded item_5.jpg (image)\n",
      "\n",
      "  Carousel position 5: Found 2 items\n",
      "    Downloaded item_1.mp4 (video)\n",
      "    Downloaded item_6.jpg (image)\n",
      "\n",
      "  Carousel position 6: Found 1 items\n",
      "    Downloaded item_7.jpg (image)\n",
      "\n",
      "  Carousel position 7: Found 1 items\n",
      "    Downloaded item_8.jpg (image)\n",
      "\n",
      "  Carousel position 8: Found 1 items\n",
      "    Downloaded item_9.jpg (image)\n",
      "\n",
      "  Carousel position 9: Found 1 items\n",
      "    Downloaded item_10.jpg (image)\n",
      "\n",
      "  Carousel position 10: Found 1 items\n",
      "    Downloaded item_11.jpg (image)\n",
      "\n",
      "  Carousel position 11: Found 1 items\n",
      "    Downloaded item_12.jpg (image)\n",
      "\n",
      "  Carousel position 12: Found 1 items\n",
      "    Downloaded item_13.jpg (image)\n",
      "\n",
      "  Carousel position 13: Found 1 items\n",
      "    Downloaded item_14.jpg (image)\n",
      "\n",
      "  Carousel position 14: Found 1 items\n",
      "    Downloaded item_15.jpg (image)\n",
      "\n",
      "  Carousel position 15: Found 1 items\n",
      "    Downloaded item_16.jpg (image)\n",
      "\n",
      "  Carousel position 16: Found 2 items\n",
      "    Downloaded item_2.mp4 (video)\n",
      "    Downloaded item_17.jpg (image)\n",
      "\n",
      "  Carousel position 17: Found 1 items\n",
      "    Downloaded item_18.jpg (image)\n",
      "\n",
      "  Carousel position 18: Found 1 items\n",
      "    Downloaded item_19.jpg (image)\n",
      "\n",
      "  Carousel position 19: Found 1 items\n",
      "    Downloaded item_20.jpg (image)\n",
      "\n",
      "  Carousel position 20: Found 1 items\n",
      "    Downloaded item_21.jpg (image)\n",
      "\n",
      "Processed 21 carousel positions\n",
      "✓ Downloaded 23 unique items to 'instagram_downloads\\brianaking\\DHG3t4VvUSu'\n",
      "\n",
      "Post 3/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/reel/Cwf061jphsT/\n",
      "============================================================\n",
      "Date: 2023-08-28 17:55:57\n",
      "Posted by: brianaking\n",
      "Stats: Likes=4.4K, Comments=95, Paid=False\n",
      "Saved caption (32 chars)\n",
      "\n",
      "  Carousel position 1: Found 3 items\n",
      "    Downloaded item_1.mp4 (video)\n",
      "    Skipped item_1: Duplicate content\n",
      "    Skipped item_1: Duplicate content\n",
      "\n",
      "Processed 1 carousel positions\n",
      "✓ Downloaded 1 unique items to 'instagram_downloads\\brianaking\\Cwf061jphsT'\n",
      "\n",
      "Post 4/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/p/DChbRcNyq5p/\n",
      "============================================================\n",
      "Date: 2024-11-18 18:10:44\n",
      "Posted by: brianaking\n",
      "Stats: Likes=2.2K, Comments=87, Paid=False\n",
      "Saved caption (117 chars)\n",
      "\n",
      "  Carousel position 1: Found 5 items\n",
      "    Downloaded item_1.mp4 (video)\n",
      "    Skipped item_1: Duplicate content\n",
      "    Downloaded item_1.jpg (image)\n",
      "    Downloaded item_2.jpg (image)\n",
      "    Skipped item_3: Duplicate content\n",
      "\n",
      "  Carousel position 3: Found 2 items\n",
      "    Downloaded item_2.mp4 (video)\n",
      "    Downloaded item_3.jpg (image)\n",
      "\n",
      "  Carousel position 4: Found 2 items\n",
      "    Downloaded item_3.mp4 (video)\n",
      "    Downloaded item_4.jpg (image)\n",
      "\n",
      "  Carousel position 5: Found 2 items\n",
      "    Downloaded item_4.mp4 (video)\n",
      "    Downloaded item_5.jpg (image)\n",
      "\n",
      "  Carousel position 6: Found 2 items\n",
      "    Downloaded item_5.mp4 (video)\n",
      "    Downloaded item_6.jpg (image)\n",
      "\n",
      "  Carousel position 7: Found 1 items\n",
      "    Downloaded item_7.jpg (image)\n",
      "\n",
      "  Carousel position 8: Found 1 items\n",
      "    Downloaded item_8.jpg (image)\n",
      "\n",
      "  Carousel position 9: Found 1 items\n",
      "    Downloaded item_9.jpg (image)\n",
      "\n",
      "  Carousel position 10: Found 1 items\n",
      "    Downloaded item_10.jpg (image)\n",
      "\n",
      "  Carousel position 11: Found 1 items\n",
      "    Downloaded item_11.jpg (image)\n",
      "\n",
      "  Carousel position 12: Found 1 items\n",
      "    Downloaded item_12.jpg (image)\n",
      "\n",
      "  Carousel position 13: Found 1 items\n",
      "    Downloaded item_13.jpg (image)\n",
      "\n",
      "  Carousel position 14: Found 1 items\n",
      "    Downloaded item_14.jpg (image)\n",
      "\n",
      "  Carousel position 15: Found 1 items\n",
      "    Downloaded item_15.jpg (image)\n",
      "\n",
      "  Carousel position 16: Found 1 items\n",
      "    Downloaded item_16.jpg (image)\n",
      "\n",
      "  Carousel position 17: Found 1 items\n",
      "    Downloaded item_17.jpg (image)\n",
      "\n",
      "  Carousel position 18: Found 1 items\n",
      "    Downloaded item_18.jpg (image)\n",
      "\n",
      "  Carousel position 19: Found 1 items\n",
      "    Downloaded item_19.jpg (image)\n",
      "\n",
      "  Carousel position 20: Found 1 items\n",
      "    Downloaded item_20.jpg (image)\n",
      "\n",
      "Processed 21 carousel positions\n",
      "✓ Downloaded 25 unique items to 'instagram_downloads\\brianaking\\DChbRcNyq5p'\n",
      "\n",
      "Post 5/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/p/C2Ksv0NLBP0/\n",
      "============================================================\n",
      "Date: 2024-01-16 16:04:10\n",
      "Posted by: brianaking\n",
      "Stats: Likes=3.2K, Comments=98, Paid=False\n",
      "Saved caption (115 chars)\n",
      "\n",
      "  Carousel position 1: Found 5 items\n",
      "    Downloaded item_1.mp4 (video)\n",
      "    Downloaded item_2.mp4 (video)\n",
      "    Skipped item_1: Duplicate content\n",
      "    Downloaded item_1.jpg (image)\n",
      "    Skipped item_2: Duplicate content\n",
      "\n",
      "  Carousel position 3: Found 2 items\n",
      "    Downloaded item_3.mp4 (video)\n",
      "    Downloaded item_2.jpg (image)\n",
      "\n",
      "  Carousel position 4: Found 2 items\n",
      "    Downloaded item_4.mp4 (video)\n",
      "    Downloaded item_3.jpg (image)\n",
      "\n",
      "  Carousel position 5: Found 2 items\n",
      "    Downloaded item_5.mp4 (video)\n",
      "    Downloaded item_4.jpg (image)\n",
      "\n",
      "  Carousel position 6: Found 2 items\n",
      "    Downloaded item_6.mp4 (video)\n",
      "    Downloaded item_5.jpg (image)\n",
      "\n",
      "  Carousel position 7: Found 1 items\n",
      "    Downloaded item_6.jpg (image)\n",
      "\n",
      "  Carousel position 8: Found 2 items\n",
      "    Downloaded item_7.mp4 (video)\n",
      "    Downloaded item_7.jpg (image)\n",
      "\n",
      "  Carousel position 9: Found 1 items\n",
      "    Downloaded item_8.jpg (image)\n",
      "\n",
      "  Carousel position 10: Found 2 items\n",
      "    Downloaded item_8.mp4 (video)\n",
      "    Downloaded item_9.jpg (image)\n",
      "\n",
      "Processed 11 carousel positions\n",
      "✓ Downloaded 17 unique items to 'instagram_downloads\\brianaking\\C2Ksv0NLBP0'\n",
      "\n",
      "Post 6/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/reel/CzOompFrFBL/\n",
      "============================================================\n",
      "Date: 2023-11-04 15:11:46\n",
      "Posted by: brianaking\n",
      "Stats: Likes=2K, Comments=63, Paid=False\n",
      "Saved caption (46 chars)\n",
      "\n",
      "  Carousel position 1: Found 3 items\n",
      "    Downloaded item_1.mp4 (video)\n",
      "    Skipped item_1: Duplicate content\n",
      "    Downloaded item_1.jpg (image)\n",
      "\n",
      "Processed 1 carousel positions\n",
      "✓ Downloaded 2 unique items to 'instagram_downloads\\brianaking\\CzOompFrFBL'\n",
      "\n",
      "Post 7/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/p/DCb_5PlvmJI/\n",
      "============================================================\n",
      "Date: 2024-11-16 15:35:18\n",
      "Posted by: brianaking\n",
      "Stats: Likes=3, Comments=48, Paid=False\n",
      "Saved caption (71 chars)\n",
      "\n",
      "  Carousel position 1: Found 3 items\n",
      "    Skipped item_1: Duplicate content\n",
      "    Downloaded item_1.jpg (image)\n",
      "    Skipped item_2: Duplicate content\n",
      "\n",
      "Processed 1 carousel positions\n",
      "✓ Downloaded 1 unique items to 'instagram_downloads\\brianaking\\DCb_5PlvmJI'\n",
      "\n",
      "Post 8/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/p/C17StY-L3Ki/\n",
      "============================================================\n",
      "Date: 2024-01-10 16:28:02\n",
      "Posted by: brianaking\n",
      "Stats: Likes=2.4K, Comments=85, Paid=False\n",
      "Saved caption (105 chars)\n",
      "\n",
      "  Carousel position 1: Found 4 items\n",
      "    Skipped item_1: Duplicate content\n",
      "    Downloaded item_1.jpg (image)\n",
      "    Downloaded item_2.jpg (image)\n",
      "    Skipped item_3: Duplicate content\n",
      "\n",
      "  Carousel position 3: Found 1 items\n",
      "    Downloaded item_3.jpg (image)\n",
      "\n",
      "  Carousel position 4: Found 1 items\n",
      "    Downloaded item_4.jpg (image)\n",
      "\n",
      "  Carousel position 5: Found 1 items\n",
      "    Downloaded item_5.jpg (image)\n",
      "\n",
      "  Carousel position 6: Found 1 items\n",
      "    Downloaded item_6.jpg (image)\n",
      "\n",
      "  Carousel position 7: Found 1 items\n",
      "    Downloaded item_7.jpg (image)\n",
      "\n",
      "  Carousel position 8: Found 1 items\n",
      "    Downloaded item_8.jpg (image)\n",
      "\n",
      "  Carousel position 9: Found 1 items\n",
      "    Downloaded item_9.jpg (image)\n",
      "\n",
      "  Carousel position 10: Found 1 items\n",
      "    Downloaded item_10.jpg (image)\n",
      "\n",
      "Processed 11 carousel positions\n",
      "✓ Downloaded 10 unique items to 'instagram_downloads\\brianaking\\C17StY-L3Ki'\n",
      "\n",
      "Post 9/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/reel/C61aBeiu5Ir/\n",
      "============================================================\n",
      "Date: 2024-05-11 16:15:15\n",
      "Posted by: brianaking\n",
      "Stats: Likes=2.6K, Comments=89, Paid=False\n",
      "Saved caption (40 chars)\n",
      "\n",
      "  Carousel position 1: Found 2 items\n",
      "    Downloaded item_1.mp4 (video)\n",
      "    Skipped item_1: Duplicate content\n",
      "\n",
      "Processed 1 carousel positions\n",
      "✓ Downloaded 1 unique items to 'instagram_downloads\\brianaking\\C61aBeiu5Ir'\n",
      "\n",
      "Post 10/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/p/C35VDe7rzQg/\n",
      "============================================================\n",
      "Date: 2024-02-28 15:12:50\n",
      "Posted by: brianaking\n",
      "Stats: Likes=1.5K, Comments=26, Paid=False\n",
      "Saved caption (35 chars)\n",
      "\n",
      "  Carousel position 1: Found 4 items\n",
      "    Skipped item_1: Duplicate content\n",
      "    Downloaded item_1.jpg (image)\n",
      "    Downloaded item_2.jpg (image)\n",
      "    Skipped item_3: Duplicate content\n",
      "\n",
      "  Carousel position 3: Found 1 items\n",
      "    Downloaded item_3.jpg (image)\n",
      "\n",
      "  Carousel position 4: Found 1 items\n",
      "    Downloaded item_4.jpg (image)\n",
      "\n",
      "Processed 5 carousel positions\n",
      "✓ Downloaded 4 unique items to 'instagram_downloads\\brianaking\\C35VDe7rzQg'\n",
      "\n",
      "Post 11/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/reel/C_dzH6_yBG2/\n",
      "============================================================\n",
      "Date: 2024-09-03 18:49:07\n",
      "Posted by: brianaking\n",
      "Stats: Likes=2K, Comments=42, Paid=False\n",
      "Saved caption (39 chars)\n",
      "\n",
      "  Carousel position 1: Found 3 items\n",
      "    Downloaded item_1.mp4 (video)\n",
      "    Skipped item_1: Duplicate content\n",
      "    Skipped item_1: Duplicate content\n",
      "\n",
      "Processed 1 carousel positions\n",
      "✓ Downloaded 1 unique items to 'instagram_downloads\\brianaking\\C_dzH6_yBG2'\n",
      "\n",
      "Post 12/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/whatshapesuspodcast/reel/DHKW3tlIsTS/\n",
      "============================================================\n",
      "Date: 2025-03-14 01:47:25\n",
      "Posted by: whatshapesuspodcast\n",
      "Stats: Likes=3, Comments=65, Paid=False\n",
      "Saved caption (116 chars)\n",
      "\n",
      "  Carousel position 1: Found 3 items\n",
      "    Downloaded item_1.mp4 (video)\n",
      "    Downloaded item_1.jpg (image)\n",
      "    Skipped item_2: Duplicate content\n",
      "\n",
      "Processed 1 carousel positions\n",
      "✓ Downloaded 2 unique items to 'instagram_downloads\\brianaking\\DHKW3tlIsTS'\n",
      "\n",
      "Post 13/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/p/DOLz9BVkTJJ/\n",
      "============================================================\n",
      "Date: 2025-09-04 14:59:50\n",
      "Posted by: brianaking\n",
      "Stats: Likes=1.2K, Comments=143, Paid=True\n",
      "Saved caption (118 chars)\n",
      "\n",
      "  Carousel position 1: Found 3 items\n",
      "    Skipped item_1: Duplicate content\n",
      "    Downloaded item_1.jpg (image)\n",
      "    Skipped item_2: Duplicate content\n",
      "\n",
      "Processed 1 carousel positions\n",
      "✓ Downloaded 1 unique items to 'instagram_downloads\\brianaking\\DOLz9BVkTJJ'\n",
      "\n",
      "Post 14/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/p/C_Lu4ZxStwx/\n",
      "============================================================\n",
      "Date: 2024-08-27 18:24:36\n",
      "Posted by: brianaking\n",
      "Stats: Likes=1.4K, Comments=32, Paid=False\n",
      "Saved caption (106 chars)\n",
      "\n",
      "  Carousel position 1: Found 4 items\n",
      "    Skipped item_1: Duplicate content\n",
      "    Downloaded item_1.jpg (image)\n",
      "    Downloaded item_2.jpg (image)\n",
      "    Skipped item_3: Duplicate content\n",
      "\n",
      "  Carousel position 3: Found 1 items\n",
      "    Downloaded item_3.jpg (image)\n",
      "\n",
      "Processed 4 carousel positions\n",
      "✓ Downloaded 3 unique items to 'instagram_downloads\\brianaking\\C_Lu4ZxStwx'\n",
      "\n",
      "Post 15/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/reel/C4ie_zHuG80/\n",
      "============================================================\n",
      "Date: 2024-03-15 14:50:30\n",
      "Posted by: brianaking\n",
      "Stats: Likes=3, Comments=27, Paid=False\n",
      "Saved caption (35 chars)\n",
      "\n",
      "  Carousel position 1: Found 3 items\n",
      "    Downloaded item_1.mp4 (video)\n",
      "    Skipped item_1: Duplicate content\n",
      "    Failed item_1: HTTPSConnectionPool(host='instagram.fmaa\n",
      "\n",
      "Processed 1 carousel positions\n",
      "✓ Downloaded 1 unique items to 'instagram_downloads\\brianaking\\C4ie_zHuG80'\n",
      "\n",
      "Post 16/240\n",
      "⏩ Skipping post from 2023-07-25 (before cutoff)\n",
      "\n",
      "Post 17/240\n",
      "⏩ Skipping post from 2022-12-22 (before cutoff)\n",
      "\n",
      "Post 18/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/p/DATsv5JyDlf/\n",
      "============================================================\n",
      "Date: 2024-09-24 17:11:17\n",
      "Posted by: brianaking\n",
      "Stats: Likes=1.2K, Comments=24, Paid=False\n",
      "Saved caption (36 chars)\n",
      "\n",
      "  Carousel position 1: Found 4 items\n",
      "    Skipped item_1: Duplicate content\n",
      "    Downloaded item_1.jpg (image)\n",
      "    Downloaded item_2.jpg (image)\n",
      "    Skipped item_3: Duplicate content\n",
      "\n",
      "  Carousel position 3: Found 1 items\n",
      "    Downloaded item_3.jpg (image)\n",
      "\n",
      "  Carousel position 4: Found 1 items\n",
      "    Downloaded item_4.jpg (image)\n",
      "\n",
      "  Carousel position 5: Found 1 items\n",
      "    Downloaded item_5.jpg (image)\n",
      "\n",
      "Processed 6 carousel positions\n",
      "✓ Downloaded 5 unique items to 'instagram_downloads\\brianaking\\DATsv5JyDlf'\n",
      "\n",
      "Post 19/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/reel/C2sKxVBR00Y/\n",
      "============================================================\n",
      "Date: 2024-01-29 16:01:58\n",
      "Posted by: brianaking\n",
      "Stats: Likes=1.1K, Comments=33, Paid=False\n",
      "Saved caption (27 chars)\n",
      "\n",
      "  Carousel position 1: Found 3 items\n",
      "    Downloaded item_1.mp4 (video)\n",
      "    Skipped item_1: Duplicate content\n",
      "    Skipped item_1: Duplicate content\n",
      "\n",
      "Processed 1 carousel positions\n",
      "✓ Downloaded 1 unique items to 'instagram_downloads\\brianaking\\C2sKxVBR00Y'\n",
      "\n",
      "Post 20/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/reel/DGLxAT5SBjG/\n",
      "============================================================\n",
      "Date: 2025-02-17 18:23:13\n",
      "Posted by: brianaking\n",
      "Stats: Likes=1.1K, Comments=51, Paid=False\n",
      "Saved caption (33 chars)\n",
      "\n",
      "  Carousel position 1: Found 3 items\n",
      "    Downloaded item_1.mp4 (video)\n",
      "    Skipped item_1: Duplicate content\n",
      "    Skipped item_1: Duplicate content\n",
      "\n",
      "Processed 1 carousel positions\n",
      "✓ Downloaded 1 unique items to 'instagram_downloads\\brianaking\\DGLxAT5SBjG'\n",
      "\n",
      "Post 21/240\n",
      "\n",
      "============================================================\n",
      "Scraping: https://www.instagram.com/brianaking/p/C-p5qNjvhK8/\n",
      "============================================================\n",
      "Date: 2024-08-14 15:04:36\n",
      "Posted by: brianaking\n",
      "Stats: Likes=1.2K, Comments=65, Paid=False\n",
      "Saved caption (115 chars)\n",
      "\n",
      "  Carousel position 1: Found 3 items\n",
      "    Skipped item_1: Duplicate content\n",
      "    Downloaded item_1.jpg (image)\n",
      "    Downloaded item_2.jpg (image)\n",
      "\n",
      "  Carousel position 3: Found 1 items\n",
      "    Downloaded item_3.jpg (image)\n",
      "\n",
      "  Carousel position 4: Found 1 items\n",
      "    Downloaded item_4.jpg (image)\n",
      "\n",
      "  Carousel position 5: Found 1 items\n",
      "    Downloaded item_5.jpg (image)\n",
      "\n",
      "  Carousel position 6: Found 1 items\n",
      "    Downloaded item_6.jpg (image)\n",
      "\n",
      "  Carousel position 7: Found 1 items\n",
      "    Downloaded item_7.jpg (image)\n",
      "\n",
      "  Carousel position 8: Found 1 items\n",
      "    Downloaded item_8.jpg (image)\n",
      "\n",
      "  Carousel position 9: Found 1 items\n",
      "    Downloaded item_9.jpg (image)\n",
      "\n",
      "  Carousel position 10: Found 1 items\n",
      "    Downloaded item_10.jpg (image)\n",
      "\n",
      "  Carousel position 11: Found 2 items\n",
      "    Downloaded item_1.mp4 (video)\n",
      "    Downloaded item_11.jpg (image)\n",
      "\n",
      "  Carousel position 12: Found 1 items\n",
      "    Downloaded item_12.jpg (image)\n",
      "\n",
      "  Carousel position 13: Found 1 items\n",
      "    Downloaded item_13.jpg (image)\n",
      "\n",
      "  Carousel position 14: Found 1 items\n",
      "    Downloaded item_14.jpg (image)\n",
      "\n",
      "  Carousel position 15: Found 1 items\n",
      "    Downloaded item_15.jpg (image)\n",
      "\n",
      "  Carousel position 16: Found 1 items\n",
      "    Downloaded item_16.jpg (image)\n"
     ]
    }
   ],
   "source": [
    "# Main scraping loop\n",
    "if USERS_TO_SCRAPE:\n",
    "    for username in USERS_TO_SCRAPE:\n",
    "        scrape_user(username, max_posts=None)\n",
    "        time.sleep(6 * SLEEP_MULTIPLIER)\n",
    "else:\n",
    "    print(\"⚠ No users to scrape. Add usernames to USERS_TO_SCRAPE list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats are now saved per user in their respective folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close browser\n",
    "driver.quit()\n",
    "print(\"\\n✓ Browser closed\")\n",
    "print(\"✓ All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mathias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
