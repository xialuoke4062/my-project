{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram Multi-User Scraper (DOM-based, Mobile View)\n",
    "Combines mobile view for stats + BeautifulSoup DOM parsing for clean media extraction\n",
    "Scrolls back to Aug 17, 2023 for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USERNAME = \"\"  # Your Instagram username\n",
    "PASSWORD = \"\"  # Your Instagram password\n",
    "\n",
    "# List of usernames to scrape\n",
    "USERS_TO_SCRAPE = []  # e.g., [\"user1\", \"user2\", \"user3\"]\n",
    "\n",
    "# Cutoff date - only scrape posts from this date forward\n",
    "CUTOFF_DATE = datetime(2023, 8, 17, 0, 0, 0)\n",
    "\n",
    "DOWNLOAD_DIR = Path(\"instagram_downloads\")\n",
    "DOWNLOAD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Global tracking\n",
    "downloaded_hashes = set()\n",
    "stats_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Chrome with mobile emulation\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('mobileEmulation', {\n",
    "    'deviceName': 'iPhone 12 Pro'\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "print(\"✓ Browser opened with mobile emulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Instagram\n",
    "def login_instagram(username, password):\n",
    "    driver.get('https://www.instagram.com/')\n",
    "    time.sleep(6)  # Doubled\n",
    "    \n",
    "    try:\n",
    "        # Wait for and click login button if on homepage\n",
    "        try:\n",
    "            login_link = WebDriverWait(driver, 10).until(  # Doubled\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//a[contains(@href, '/accounts/login')]\"))\n",
    "            )\n",
    "            login_link.click()\n",
    "            time.sleep(4)  # Doubled\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Enter username\n",
    "        username_input = WebDriverWait(driver, 20).until(  # Doubled\n",
    "            EC.presence_of_element_located((By.NAME, \"username\"))\n",
    "        )\n",
    "        username_input.send_keys(username)\n",
    "        \n",
    "        # Enter password\n",
    "        password_input = driver.find_element(By.NAME, \"password\")\n",
    "        password_input.send_keys(password)\n",
    "        password_input.send_keys(Keys.RETURN)\n",
    "        \n",
    "        time.sleep(10)  # Doubled\n",
    "        \n",
    "        # Handle \"Save Your Login Info\" popup\n",
    "        try:\n",
    "            not_now = WebDriverWait(driver, 10).until(  # Doubled\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not now') or contains(text(), 'Not Now')]\"))\n",
    "            )\n",
    "            not_now.click()\n",
    "            time.sleep(4)  # Doubled\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Handle \"Turn on Notifications\" popup\n",
    "        try:\n",
    "            not_now = WebDriverWait(driver, 10).until(  # Doubled\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not Now')]\"))\n",
    "            )\n",
    "            not_now.click()\n",
    "            time.sleep(4)  # Doubled\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(\"✓ Logged in successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Perform login\n",
    "if USERNAME and PASSWORD:\n",
    "    login_instagram(USERNAME, PASSWORD)\n",
    "else:\n",
    "    print(\"⚠ No login credentials provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract post date\n",
    "def extract_post_date():\n",
    "    try:\n",
    "        # Look for time element with class x1p4m5qa\n",
    "        time_elements = driver.find_elements(By.CSS_SELECTOR, 'time.x1p4m5qa')\n",
    "        if time_elements:\n",
    "            datetime_str = time_elements[0].get_attribute('datetime')\n",
    "            if datetime_str:\n",
    "                # Parse ISO format datetime\n",
    "                post_date = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))\n",
    "                return post_date\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not extract date: {e}\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract post stats (works in mobile view)\n",
    "def extract_post_stats():\n",
    "    likes = \"0\"\n",
    "    comments = \"0\"\n",
    "    is_paid = False\n",
    "    \n",
    "    try:\n",
    "        # Get likes - mobile view selector\n",
    "        like_spans = driver.find_elements(By.CSS_SELECTOR, 'span.x1ypdohk.x1s688f.x2fvf9.xe9ewy2[role=\"button\"]')\n",
    "        if like_spans:\n",
    "            likes = like_spans[0].text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Get comments - mobile view selector\n",
    "        comment_spans = driver.find_elements(\n",
    "            By.CSS_SELECTOR,\n",
    "            'span.xdj266r.x14z9mp.xat24cr.x1lziwak.xexx8yu.xyri2b.x18d9i69.x1c1uobl.x1hl2dhg.x16tdsg8.x1vvkbs'\n",
    "        )\n",
    "        for span in comment_spans:\n",
    "            text = span.text\n",
    "            if text.replace(',', '').isdigit():\n",
    "                comments = text\n",
    "                break\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Check for paid partnership\n",
    "        if \"Paid partnership with \" in driver.page_source:\n",
    "            is_paid = True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return likes, comments, is_paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract media URLs using BeautifulSoup DOM parsing (NO network logs!)\n",
    "def extract_media_urls_dom():\n",
    "    media_urls = set()\n",
    "    \n",
    "    try:\n",
    "        # Get all img tags\n",
    "        images = driver.find_elements(By.TAG_NAME, 'img')\n",
    "        for img in images:\n",
    "            src = img.get_attribute('src')\n",
    "            if src and ('cdninstagram.com' in src or 'fbcdn.net' in src):\n",
    "                # Filter out small thumbnails and profile pics\n",
    "                if '/s150x150/' not in src and '/s320x320/' not in src and 's640x640' not in src:\n",
    "                    media_urls.add(src)\n",
    "        \n",
    "        # Get all video tags\n",
    "        videos = driver.find_elements(By.TAG_NAME, 'video')\n",
    "        for video in videos:\n",
    "            src = video.get_attribute('src')\n",
    "            if src and ('cdninstagram.com' in src or 'fbcdn.net' in src):\n",
    "                media_urls.add(src)\n",
    "        \n",
    "        # Alternative: use BeautifulSoup on entire page\n",
    "        if len(media_urls) == 0:\n",
    "            soup = bs(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Find videos\n",
    "            for video in soup.find_all('video'):\n",
    "                src = video.get('src')\n",
    "                if src and ('cdninstagram.com' in src or 'fbcdn.net' in src):\n",
    "                    media_urls.add(src)\n",
    "            \n",
    "            # Find images\n",
    "            for img in soup.find_all('img'):\n",
    "                src = img.get('src')\n",
    "                if src and ('cdninstagram.com' in src or 'fbcdn.net' in src):\n",
    "                    if '/s150x150/' not in src and '/s320x320/' not in src and 's640x640' not in src:\n",
    "                        media_urls.add(src)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error extracting media: {e}\")\n",
    "    \n",
    "    return media_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click through carousel\n",
    "def click_through_carousel():\n",
    "    click_count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            # Try multiple selectors for next button\n",
    "            next_btn = None\n",
    "            selectors = [\n",
    "                'button[aria-label=\"Next\"]',\n",
    "                'button[aria-label=\"next\"]',\n",
    "                'button._afxw._al46._al47'\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    next_btn = WebDriverWait(driver, 2).until(  # Doubled\n",
    "                        EC.element_to_be_clickable((By.CSS_SELECTOR, selector))\n",
    "                    )\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if next_btn:\n",
    "                next_btn.click()\n",
    "                click_count += 1\n",
    "                time.sleep(1)  # Doubled\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "            break\n",
    "    \n",
    "    return click_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download media with duplicate detection and 5 second pauses\n",
    "def download_media(media_urls, post_dir):\n",
    "    success_count = 0\n",
    "    \n",
    "    for idx, url in enumerate(media_urls, 1):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Check for duplicates by content hash\n",
    "            content_hash = hashlib.md5(response.content).hexdigest()\n",
    "            if content_hash in downloaded_hashes:\n",
    "                print(f\"  Skipped {idx}/{len(media_urls)}: Duplicate\")\n",
    "                time.sleep(5)  # Added 5 second pause\n",
    "                continue\n",
    "            \n",
    "            downloaded_hashes.add(content_hash)\n",
    "            \n",
    "            # Determine file extension\n",
    "            content_type = response.headers.get('content-type', '')\n",
    "            if 'video' in content_type:\n",
    "                ext = '.mp4'\n",
    "            elif 'image' in content_type:\n",
    "                ext = '.jpg'\n",
    "            else:\n",
    "                ext = '.jpg'  # default\n",
    "            \n",
    "            success_count += 1\n",
    "            filename = post_dir / f\"item_{success_count}{ext}\"\n",
    "            \n",
    "            filename.write_bytes(response.content)\n",
    "            print(f\"  Downloaded {success_count}: {filename.name}\")\n",
    "            \n",
    "            # 5 second pause between downloads\n",
    "            time.sleep(5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Skipped {idx}/{len(media_urls)}: {str(e)[:40]}\")\n",
    "            time.sleep(5)  # Added 5 second pause\n",
    "    \n",
    "    return success_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape a single post\n",
    "def scrape_post(post_url):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping: {post_url}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    driver.get(post_url)\n",
    "    time.sleep(6)  # Doubled\n",
    "    \n",
    "    # Extract date\n",
    "    post_date = extract_post_date()\n",
    "    if post_date:\n",
    "        print(f\"Date: {post_date.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Extract stats\n",
    "    likes, comments, is_paid = extract_post_stats()\n",
    "    print(f\"Stats: Likes={likes}, Comments={comments}, Paid={is_paid}\")\n",
    "    \n",
    "    # Click through carousel to load all media\n",
    "    click_count = click_through_carousel()\n",
    "    if click_count > 0:\n",
    "        print(f\"Clicked next {click_count} times\")\n",
    "    \n",
    "    # Extract media URLs using DOM parsing\n",
    "    media_urls = extract_media_urls_dom()\n",
    "    print(f\"Found {len(media_urls)} media items\")\n",
    "    \n",
    "    # Create directory for this post\n",
    "    post_id = post_url.rstrip('/').split('/')[-1]\n",
    "    post_dir = DOWNLOAD_DIR / post_id\n",
    "    post_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Download media\n",
    "    success_count = download_media(media_urls, post_dir)\n",
    "    \n",
    "    # Log stats\n",
    "    stats_log.append({\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'post_url': post_url,\n",
    "        'post_id': post_id,\n",
    "        'post_date': post_date.isoformat() if post_date else None,\n",
    "        'likes': likes,\n",
    "        'comments': comments,\n",
    "        'paid_partnership': is_paid,\n",
    "        'media_downloaded': success_count\n",
    "    })\n",
    "    \n",
    "    print(f\"✓ Downloaded {success_count} unique items to '{post_dir}'\")\n",
    "    return success_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape user profile - scroll back to Aug 17, 2023\n",
    "def scrape_user(username, max_posts=None):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping user: {username}\")\n",
    "    print(f\"Cutoff date: {CUTOFF_DATE.strftime('%Y-%m-%d')}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    profile_url = f\"https://www.instagram.com/{username}/\"\n",
    "    driver.get(profile_url)\n",
    "    time.sleep(6)  # Doubled\n",
    "    \n",
    "    # Scroll to load posts until we reach cutoff date\n",
    "    scroll_count = 0\n",
    "    reached_cutoff = False\n",
    "    \n",
    "    while not reached_cutoff:\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(4)  # Doubled\n",
    "        scroll_count += 1\n",
    "        \n",
    "        # Check if we've loaded posts old enough\n",
    "        # Look for time elements on the page\n",
    "        try:\n",
    "            time_elements = driver.find_elements(By.CSS_SELECTOR, 'time')\n",
    "            if time_elements:\n",
    "                # Check the last (oldest visible) post date\n",
    "                for time_elem in reversed(time_elements[-5:]):  # Check last 5 time elements\n",
    "                    try:\n",
    "                        datetime_str = time_elem.get_attribute('datetime')\n",
    "                        if datetime_str:\n",
    "                            post_date = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))\n",
    "                            # Remove timezone info for comparison\n",
    "                            post_date_naive = post_date.replace(tzinfo=None)\n",
    "                            \n",
    "                            if post_date_naive < CUTOFF_DATE:\n",
    "                                print(f\"Reached cutoff date after {scroll_count} scrolls\")\n",
    "                                print(f\"Oldest post date: {post_date_naive.strftime('%Y-%m-%d')}\")\n",
    "                                reached_cutoff = True\n",
    "                                break\n",
    "                    except:\n",
    "                        continue\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Safety limit - stop after 100 scrolls\n",
    "        if scroll_count >= 100:\n",
    "            print(f\"Reached scroll limit (100 scrolls)\")\n",
    "            break\n",
    "        \n",
    "        if scroll_count % 10 == 0:\n",
    "            print(f\"  Scrolled {scroll_count} times...\")\n",
    "    \n",
    "    print(f\"Total scrolls: {scroll_count}\")\n",
    "    \n",
    "    # Find all post links\n",
    "    try:\n",
    "        post_links = []\n",
    "        \n",
    "        # Find all links with /p/ (posts)\n",
    "        links = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"/p/\"]')\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get_attribute('href')\n",
    "            if href and '/p/' in href:\n",
    "                post_links.append(href)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        post_links = list(dict.fromkeys(post_links))\n",
    "        \n",
    "        if max_posts:\n",
    "            post_links = post_links[:max_posts]\n",
    "        \n",
    "        print(f\"Found {len(post_links)} posts\")\n",
    "        \n",
    "        # Scrape each post\n",
    "        total_downloaded = 0\n",
    "        for i, post_url in enumerate(post_links, 1):\n",
    "            print(f\"\\nPost {i}/{len(post_links)}\")\n",
    "            count = scrape_post(post_url)\n",
    "            total_downloaded += count\n",
    "            time.sleep(4)  # Doubled\n",
    "        \n",
    "        print(f\"\\n✓ User '{username}' complete: {total_downloaded} total items downloaded\")\n",
    "        return total_downloaded\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping user: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main scraping loop - Scrape all users in the list\n",
    "if USERS_TO_SCRAPE:\n",
    "    for username in USERS_TO_SCRAPE:\n",
    "        scrape_user(username, max_posts=None)  # Set max_posts if needed\n",
    "        time.sleep(6)  # Doubled\n",
    "else:\n",
    "    print(\"⚠ No users to scrape. Add usernames to USERS_TO_SCRAPE list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stats log\n",
    "if stats_log:\n",
    "    stats_file = DOWNLOAD_DIR / \"scrape_stats.json\"\n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(stats_log, f, indent=2)\n",
    "    print(f\"\\n✓ Stats saved to {stats_file}\")\n",
    "    print(f\"\\nTotal stats:\")\n",
    "    print(f\"  Posts scraped: {len(stats_log)}\")\n",
    "    print(f\"  Items downloaded: {sum(s['media_downloaded'] for s in stats_log)}\")\n",
    "    print(f\"  Paid partnerships: {sum(1 for s in stats_log if s['paid_partnership'])}\")\n",
    "else:\n",
    "    print(\"No stats to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close browser\n",
    "driver.quit()\n",
    "print(\"\\n✓ Browser closed\")\n",
    "print(\"✓ All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
