{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram Multi-User Scraper (DOM-based, Mobile View)\n",
    "Item-by-item downloading with smart scroll estimation\n",
    "Downloads each carousel item sequentially to capture all videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USERNAME = \"\"  # Your Instagram username\n",
    "PASSWORD = \"\"  # Your Instagram password\n",
    "\n",
    "# List of usernames to scrape\n",
    "USERS_TO_SCRAPE = []  # e.g., [\"user1\", \"user2\", \"user3\"]\n",
    "\n",
    "# Cutoff date - only scrape posts from this date forward\n",
    "CUTOFF_DATE = datetime(2023, 8, 17, 0, 0, 0)\n",
    "\n",
    "DOWNLOAD_DIR = Path(\"instagram_downloads\")\n",
    "DOWNLOAD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Global tracking\n",
    "downloaded_hashes = set()\n",
    "stats_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Chrome with mobile emulation\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('mobileEmulation', {\n",
    "    'deviceName': 'iPhone 12 Pro'\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "print(\"✓ Browser opened with mobile emulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Instagram\n",
    "def login_instagram(username, password):\n",
    "    driver.get('https://www.instagram.com/')\n",
    "    time.sleep(6)\n",
    "    \n",
    "    try:\n",
    "        # Wait for and click login button if on homepage\n",
    "        try:\n",
    "            login_link = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//a[contains(@href, '/accounts/login')]\"))\n",
    "            )\n",
    "            login_link.click()\n",
    "            time.sleep(4)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Enter username\n",
    "        username_input = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"username\"))\n",
    "        )\n",
    "        username_input.send_keys(username)\n",
    "        \n",
    "        # Enter password\n",
    "        password_input = driver.find_element(By.NAME, \"password\")\n",
    "        password_input.send_keys(password)\n",
    "        password_input.send_keys(Keys.RETURN)\n",
    "        \n",
    "        time.sleep(10)\n",
    "        \n",
    "        # Handle \"Save Your Login Info\" popup\n",
    "        try:\n",
    "            not_now = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not now') or contains(text(), 'Not Now')]\"))\n",
    "            )\n",
    "            not_now.click()\n",
    "            time.sleep(4)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Handle \"Turn on Notifications\" popup\n",
    "        try:\n",
    "            not_now = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not Now')]\"))\n",
    "            )\n",
    "            not_now.click()\n",
    "            time.sleep(4)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(\"✓ Logged in successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Perform login\n",
    "if USERNAME and PASSWORD:\n",
    "    login_instagram(USERNAME, PASSWORD)\n",
    "else:\n",
    "    print(\"⚠ No login credentials provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract post date from a post page\n",
    "def extract_post_date():\n",
    "    try:\n",
    "        time_elements = driver.find_elements(By.CSS_SELECTOR, 'time.x1p4m5qa')\n",
    "        if time_elements:\n",
    "            datetime_str = time_elements[0].get_attribute('datetime')\n",
    "            if datetime_str:\n",
    "                post_date = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))\n",
    "                return post_date.replace(tzinfo=None)\n",
    "    except:\n",
    "        pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract post caption/text\n",
    "def extract_post_caption():\n",
    "    try:\n",
    "        # Look for h1 with caption text\n",
    "        caption_elements = driver.find_elements(By.CSS_SELECTOR, 'h1._ap3a._aaco._aacu._aacx._aad7._aade')\n",
    "        if caption_elements:\n",
    "            return caption_elements[0].text\n",
    "    except:\n",
    "        pass\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample posts to get their dates\n",
    "def sample_post_dates(post_links, sample_count=3):\n",
    "    dates = []\n",
    "    sample_links = post_links[-sample_count:] if len(post_links) >= sample_count else post_links\n",
    "    \n",
    "    for post_url in sample_links:\n",
    "        try:\n",
    "            driver.get(post_url)\n",
    "            time.sleep(3)\n",
    "            post_date = extract_post_date()\n",
    "            if post_date:\n",
    "                dates.append(post_date)\n",
    "                print(f\"  Sampled: {post_date.strftime('%Y-%m-%d')}\")\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate scrolls needed to reach cutoff date\n",
    "def estimate_scrolls_needed(username):\n",
    "    print(\"\\nEstimating scrolls needed...\")\n",
    "    \n",
    "    profile_url = f\"https://www.instagram.com/{username}/\"\n",
    "    driver.get(profile_url)\n",
    "    time.sleep(6)\n",
    "    \n",
    "    # Scroll 1 time\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(4)\n",
    "    \n",
    "    links_1_scroll = []\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"/p/\"]')\n",
    "    for elem in elements:\n",
    "        href = elem.get_attribute('href')\n",
    "        if href and '/p/' in href:\n",
    "            links_1_scroll.append(href)\n",
    "    links_1_scroll = list(dict.fromkeys(links_1_scroll))\n",
    "    \n",
    "    print(f\"After 1 scroll: {len(links_1_scroll)} posts loaded\")\n",
    "    print(\"Sampling posts after 1 scroll...\")\n",
    "    dates_1_scroll = sample_post_dates(links_1_scroll, sample_count=3)\n",
    "    \n",
    "    if not dates_1_scroll:\n",
    "        print(\"Could not sample dates, using default scroll count\")\n",
    "        return 50\n",
    "    \n",
    "    earliest_1_scroll = min(dates_1_scroll)\n",
    "    latest_1_scroll = max(dates_1_scroll)\n",
    "    print(f\"Date range after 1 scroll: {latest_1_scroll.strftime('%Y-%m-%d')} to {earliest_1_scroll.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Go back and scroll 3 times\n",
    "    driver.get(profile_url)\n",
    "    time.sleep(6)\n",
    "    \n",
    "    for i in range(3):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(4)\n",
    "    \n",
    "    links_3_scroll = []\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"/p/\"]')\n",
    "    for elem in elements:\n",
    "        href = elem.get_attribute('href')\n",
    "        if href and '/p/' in href:\n",
    "            links_3_scroll.append(href)\n",
    "    links_3_scroll = list(dict.fromkeys(links_3_scroll))\n",
    "    \n",
    "    print(f\"After 3 scrolls: {len(links_3_scroll)} posts loaded\")\n",
    "    print(\"Sampling posts after 3 scrolls...\")\n",
    "    dates_3_scroll = sample_post_dates(links_3_scroll, sample_count=3)\n",
    "    \n",
    "    if not dates_3_scroll:\n",
    "        print(\"Could not sample dates, using default scroll count\")\n",
    "        return 50\n",
    "    \n",
    "    earliest_3_scroll = min(dates_3_scroll)\n",
    "    print(f\"Earliest date after 3 scrolls: {earliest_3_scroll.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    time_diff = earliest_1_scroll - earliest_3_scroll\n",
    "    days_per_2_scrolls = time_diff.days\n",
    "    \n",
    "    if days_per_2_scrolls <= 0:\n",
    "        print(\"Time difference too small, using default scroll count\")\n",
    "        return 50\n",
    "    \n",
    "    print(f\"Time covered by 2 scrolls: {days_per_2_scrolls} days\")\n",
    "    \n",
    "    days_to_cutoff = (earliest_3_scroll - CUTOFF_DATE).days\n",
    "    \n",
    "    if days_to_cutoff <= 0:\n",
    "        print(f\"Already reached cutoff date!\")\n",
    "        return 3\n",
    "    \n",
    "    additional_scrolls = int((days_to_cutoff / days_per_2_scrolls) * 2) + 5\n",
    "    total_scrolls = 3 + additional_scrolls\n",
    "    \n",
    "    print(f\"Days to cutoff: {days_to_cutoff}\")\n",
    "    print(f\"Estimated total scrolls needed: {total_scrolls}\")\n",
    "    \n",
    "    if total_scrolls > 200:\n",
    "        print(\"Capping at 200 scrolls\")\n",
    "        return 200\n",
    "    \n",
    "    return total_scrolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract post stats\n",
    "def extract_post_stats():\n",
    "    likes = \"0\"\n",
    "    comments = \"0\"\n",
    "    is_paid = False\n",
    "    \n",
    "    try:\n",
    "        like_spans = driver.find_elements(By.CSS_SELECTOR, 'span.x1ypdohk.x1s688f.x2fvf9.xe9ewy2[role=\"button\"]')\n",
    "        if like_spans:\n",
    "            likes = like_spans[0].text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        comment_spans = driver.find_elements(\n",
    "            By.CSS_SELECTOR,\n",
    "            'span.xdj266r.x14z9mp.xat24cr.x1lziwak.xexx8yu.xyri2b.x18d9i69.x1c1uobl.x1hl2dhg.x16tdsg8.x1vvkbs'\n",
    "        )\n",
    "        for span in comment_spans:\n",
    "            text = span.text\n",
    "            if text.replace(',', '').isdigit():\n",
    "                comments = text\n",
    "                break\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if \"Paid partnership with \" in driver.page_source:\n",
    "            is_paid = True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return likes, comments, is_paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract current carousel item media URL\n",
    "def extract_current_item_url():\n",
    "    try:\n",
    "        # Check for video first (priority)\n",
    "        videos = driver.find_elements(By.TAG_NAME, 'video')\n",
    "        for video in videos:\n",
    "            src = video.get_attribute('src')\n",
    "            if src and ('cdninstagram.com' in src or 'fbcdn.net' in src):\n",
    "                return src, 'video'\n",
    "        \n",
    "        # If no video, get image (but skip thumbnails and static)\n",
    "        images = driver.find_elements(By.TAG_NAME, 'img')\n",
    "        best_image = None\n",
    "        best_size = 0\n",
    "        \n",
    "        for img in images:\n",
    "            src = img.get_attribute('src')\n",
    "            if src and ('cdninstagram.com' in src or 'fbcdn.net' in src):\n",
    "                # Filter out unwanted images\n",
    "                if any(x in src for x in ['/s150x150/', '/s320x320/', 's640x640', 'static']):\n",
    "                    continue\n",
    "                \n",
    "                # Try to estimate size (larger is better)\n",
    "                try:\n",
    "                    width = img.size.get('width', 0)\n",
    "                    height = img.size.get('height', 0)\n",
    "                    size = width * height\n",
    "                    if size > best_size:\n",
    "                        best_size = size\n",
    "                        best_image = src\n",
    "                except:\n",
    "                    if not best_image:\n",
    "                        best_image = src\n",
    "        \n",
    "        if best_image:\n",
    "            return best_image, 'image'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error extracting media: {e}\")\n",
    "    \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if next button exists\n",
    "def has_next_button():\n",
    "    try:\n",
    "        selectors = [\n",
    "            'button[aria-label=\"Next\"]',\n",
    "            'button[aria-label=\"next\"]',\n",
    "            'button._afxw._al46._al47'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                next_btn = WebDriverWait(driver, 2).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, selector))\n",
    "                )\n",
    "                return next_btn\n",
    "            except:\n",
    "                continue\n",
    "        return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download single file\n",
    "def download_file(url, filepath):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check for duplicates\n",
    "        content_hash = hashlib.md5(response.content).hexdigest()\n",
    "        if content_hash in downloaded_hashes:\n",
    "            return False, \"duplicate\"\n",
    "        \n",
    "        downloaded_hashes.add(content_hash)\n",
    "        filepath.write_bytes(response.content)\n",
    "        return True, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, str(e)[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape a single post (item-by-item download)\n",
    "def scrape_post(post_url):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping: {post_url}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    driver.get(post_url)\n",
    "    time.sleep(6)\n",
    "    \n",
    "    # Extract date\n",
    "    post_date = extract_post_date()\n",
    "    if post_date:\n",
    "        print(f\"Date: {post_date.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Extract stats\n",
    "    likes, comments, is_paid = extract_post_stats()\n",
    "    print(f\"Stats: Likes={likes}, Comments={comments}, Paid={is_paid}\")\n",
    "    \n",
    "    # Extract caption\n",
    "    caption = extract_post_caption()\n",
    "    \n",
    "    # Create directory for this post\n",
    "    post_id = post_url.rstrip('/').split('/')[-1]\n",
    "    post_dir = DOWNLOAD_DIR / post_id\n",
    "    post_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save caption\n",
    "    if caption:\n",
    "        caption_file = post_dir / \"caption.txt\"\n",
    "        caption_file.write_text(caption, encoding='utf-8')\n",
    "        print(f\"Saved caption ({len(caption)} chars)\")\n",
    "    \n",
    "    # Download carousel items one by one\n",
    "    item_number = 1\n",
    "    success_count = 0\n",
    "    \n",
    "    while True:\n",
    "        # Extract current item\n",
    "        url, media_type = extract_current_item_url()\n",
    "        \n",
    "        if url:\n",
    "            ext = '.mp4' if media_type == 'video' else '.jpg'\n",
    "            filepath = post_dir / f\"item_{item_number}{ext}\"\n",
    "            \n",
    "            success, error = download_file(url, filepath)\n",
    "            if success:\n",
    "                print(f\"  Downloaded item_{item_number}{ext} ({media_type})\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                if error == \"duplicate\":\n",
    "                    print(f\"  Skipped item_{item_number}: Duplicate\")\n",
    "                else:\n",
    "                    print(f\"  Failed item_{item_number}: {error}\")\n",
    "            \n",
    "            time.sleep(5)  # 5 second pause\n",
    "        \n",
    "        # Try to click next\n",
    "        next_btn = has_next_button()\n",
    "        if not next_btn:\n",
    "            break\n",
    "        \n",
    "        next_btn.click()\n",
    "        time.sleep(1)\n",
    "        item_number += 1\n",
    "    \n",
    "    print(f\"Processed {item_number} carousel items\")\n",
    "    \n",
    "    # Log stats\n",
    "    stats_log.append({\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'post_url': post_url,\n",
    "        'post_id': post_id,\n",
    "        'post_date': post_date.isoformat() if post_date else None,\n",
    "        'likes': likes,\n",
    "        'comments': comments,\n",
    "        'paid_partnership': is_paid,\n",
    "        'caption_length': len(caption) if caption else 0,\n",
    "        'carousel_items': item_number,\n",
    "        'media_downloaded': success_count\n",
    "    })\n",
    "    \n",
    "    print(f\"✓ Downloaded {success_count} unique items to '{post_dir}'\")\n",
    "    return success_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape user profile\n",
    "def scrape_user(username, max_posts=None):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping user: {username}\")\n",
    "    print(f\"Cutoff date: {CUTOFF_DATE.strftime('%Y-%m-%d')}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Estimate scrolls needed\n",
    "    estimated_scrolls = estimate_scrolls_needed(username)\n",
    "    \n",
    "    # Scroll profile\n",
    "    print(f\"\\nScrolling {estimated_scrolls} times...\")\n",
    "    profile_url = f\"https://www.instagram.com/{username}/\"\n",
    "    driver.get(profile_url)\n",
    "    time.sleep(6)\n",
    "    \n",
    "    for i in range(estimated_scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(4)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Scrolled {i + 1}/{estimated_scrolls} times...\")\n",
    "    \n",
    "    print(f\"Completed {estimated_scrolls} scrolls\")\n",
    "    \n",
    "    # Find all post links\n",
    "    try:\n",
    "        post_links = []\n",
    "        links = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"/p/\"]')\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get_attribute('href')\n",
    "            if href and '/p/' in href:\n",
    "                post_links.append(href)\n",
    "        \n",
    "        post_links = list(dict.fromkeys(post_links))\n",
    "        \n",
    "        if max_posts:\n",
    "            post_links = post_links[:max_posts]\n",
    "        \n",
    "        print(f\"Found {len(post_links)} posts\")\n",
    "        \n",
    "        # Scrape each post\n",
    "        total_downloaded = 0\n",
    "        for i, post_url in enumerate(post_links, 1):\n",
    "            print(f\"\\nPost {i}/{len(post_links)}\")\n",
    "            count = scrape_post(post_url)\n",
    "            total_downloaded += count\n",
    "            time.sleep(4)\n",
    "        \n",
    "        print(f\"\\n✓ User '{username}' complete: {total_downloaded} total items downloaded\")\n",
    "        return total_downloaded\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping user: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main scraping loop\n",
    "if USERS_TO_SCRAPE:\n",
    "    for username in USERS_TO_SCRAPE:\n",
    "        scrape_user(username, max_posts=None)\n",
    "        time.sleep(6)\n",
    "else:\n",
    "    print(\"⚠ No users to scrape. Add usernames to USERS_TO_SCRAPE list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stats log\n",
    "if stats_log:\n",
    "    stats_file = DOWNLOAD_DIR / \"scrape_stats.json\"\n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(stats_log, f, indent=2)\n",
    "    print(f\"\\n✓ Stats saved to {stats_file}\")\n",
    "    print(f\"\\nTotal stats:\")\n",
    "    print(f\"  Posts scraped: {len(stats_log)}\")\n",
    "    print(f\"  Items downloaded: {sum(s['media_downloaded'] for s in stats_log)}\")\n",
    "    print(f\"  Paid partnerships: {sum(1 for s in stats_log if s['paid_partnership'])}\")\n",
    "else:\n",
    "    print(\"No stats to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close browser\n",
    "driver.quit()\n",
    "print(\"\\n✓ Browser closed\")\n",
    "print(\"✓ All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
