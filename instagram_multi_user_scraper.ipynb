{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Instagram Multi-User Scraper (DOM-based, Mobile View)\nItem-by-item downloading with smart scroll estimation\nDownloads ALL valid images/videos at each carousel position (URL-tracked globally to avoid duplicates)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nUSERNAME = \"\"  # Your Instagram username\nPASSWORD = \"\"  # Your Instagram password\n\n# List of usernames to scrape\nUSERS_TO_SCRAPE = []  # e.g., [\"user1\", \"user2\", \"user3\"]\n\n# Cutoff date - only scrape posts from this date forward\nCUTOFF_DATE = datetime(2023, 8, 17, 0, 0, 0)\n\n# Sleep multiplier - set to 2 or 3 to slow down (default 1)\nSLEEP_MULTIPLIER = 1  # Change to 2 or 3 if you need slower execution\n\nDOWNLOAD_DIR = Path(\"instagram_downloads\")\nDOWNLOAD_DIR.mkdir(exist_ok=True)\n\n# Global tracking\ndownloaded_hashes = set()\nseen_urls = set()  # Track image URLs globally to avoid re-downloading\nstats_log = []"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Chrome with mobile emulation\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('mobileEmulation', {\n",
    "    'deviceName': 'iPhone 12 Pro'\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "print(\"✓ Browser opened with mobile emulation\")\n",
    "print(f\"Sleep multiplier: {SLEEP_MULTIPLIER}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to Instagram\ndef login_instagram(username, password):\n    driver.get('https://www.instagram.com/')\n    \n    try:\n        # Wait for and click login button if on homepage\n        try:\n            login_link = WebDriverWait(driver, 10).until(\n                EC.element_to_be_clickable((By.XPATH, \"//a[contains(@href, '/accounts/login')]\"))\n            )\n            login_link.click()\n        except:\n            pass\n        \n        # Enter username\n        username_input = WebDriverWait(driver, 20).until(\n            EC.presence_of_element_located((By.NAME, \"username\"))\n        )\n        username_input.send_keys(username)\n        \n        # Enter password\n        password_input = driver.find_element(By.NAME, \"password\")\n        password_input.send_keys(password)\n        password_input.send_keys(Keys.RETURN)\n        \n        # Handle \"Save Your Login Info\" popup\n        try:\n            not_now = WebDriverWait(driver, 10).until(\n                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not now') or contains(text(), 'Not Now')]\"))\n            )\n            not_now.click()\n        except:\n            pass\n        \n        # Handle \"Turn on Notifications\" popup\n        try:\n            not_now = WebDriverWait(driver, 10).until(\n                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not Now')]\"))\n            )\n            not_now.click()\n        except:\n            pass\n        \n        print(\"✓ Logged in successfully\")\n        return True\n        \n    except Exception as e:\n        print(f\"Login failed: {e}\")\n        return False\n\n# Perform login\nif USERNAME and PASSWORD:\n    login_instagram(USERNAME, PASSWORD)\nelse:\n    print(\"⚠ No login credentials provided\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract post date from a post page\n",
    "def extract_post_date():\n",
    "    try:\n",
    "        time_elements = driver.find_elements(By.CSS_SELECTOR, 'time.x1p4m5qa')\n",
    "        if time_elements:\n",
    "            datetime_str = time_elements[0].get_attribute('datetime')\n",
    "            if datetime_str:\n",
    "                post_date = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))\n",
    "                return post_date.replace(tzinfo=None)\n",
    "    except:\n",
    "        pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract post caption/text\n",
    "def extract_post_caption():\n",
    "    try:\n",
    "        # Look for h1 with caption text\n",
    "        caption_elements = driver.find_elements(By.CSS_SELECTOR, 'h1._ap3a._aaco._aacu._aacx._aad7._aade')\n",
    "        if caption_elements:\n",
    "            return caption_elements[0].text\n",
    "    except:\n",
    "        pass\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# (sample_post_dates function removed - no longer needed)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Estimate scrolls needed to reach cutoff date\ndef estimate_scrolls_needed(username):\n    print(\"\\nEstimating scrolls needed...\")\n    \n    profile_url = f\"https://www.instagram.com/{username}/\"\n    driver.get(profile_url)\n    time.sleep(6 * SLEEP_MULTIPLIER)\n    \n    # Scroll 1 time and collect links\n    all_links_1_scroll = []\n    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n    time.sleep(4 * SLEEP_MULTIPLIER)\n    \n    elements = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"/p/\"], a[href*=\"/reel/\"]')\n    seen = set()\n    for elem in elements:\n        href = elem.get_attribute('href')\n        if href and ('/p/' in href or '/reel/' in href) and href not in seen:\n            all_links_1_scroll.append(href)\n            seen.add(href)\n    \n    print(f\"After 1 scroll: {len(all_links_1_scroll)} posts collected\")\n    \n    if not all_links_1_scroll:\n        print(\"No posts found, using default scroll count\")\n        return 50\n    \n    # Get date from last post (oldest visible)\n    print(\"Checking date of last post after 1 scroll...\")\n    driver.get(all_links_1_scroll[-1])\n    time.sleep(3 * SLEEP_MULTIPLIER)\n    earliest_1_scroll = extract_post_date()\n    \n    if not earliest_1_scroll:\n        print(\"Could not extract date, using default scroll count\")\n        return 50\n    \n    print(f\"Earliest date after 1 scroll: {earliest_1_scroll.strftime('%Y-%m-%d')}\")\n    \n    # Go back and scroll 3 times, collecting links after each scroll\n    driver.get(profile_url)\n    time.sleep(6 * SLEEP_MULTIPLIER)\n    \n    all_links_3_scroll = []\n    seen = set()\n    for i in range(3):\n        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n        time.sleep(4 * SLEEP_MULTIPLIER)\n        \n        # Collect links after each scroll\n        elements = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"/p/\"], a[href*=\"/reel/\"]')\n        for elem in elements:\n            href = elem.get_attribute('href')\n            if href and ('/p/' in href or '/reel/' in href) and href not in seen:\n                all_links_3_scroll.append(href)\n                seen.add(href)\n    \n    print(f\"After 3 scrolls: {len(all_links_3_scroll)} posts collected\")\n    \n    if not all_links_3_scroll:\n        print(\"No posts found, using default scroll count\")\n        return 50\n    \n    # Get date from last post (oldest visible)\n    print(\"Checking date of last post after 3 scrolls...\")\n    driver.get(all_links_3_scroll[-1])\n    time.sleep(3 * SLEEP_MULTIPLIER)\n    earliest_3_scroll = extract_post_date()\n    \n    if not earliest_3_scroll:\n        print(\"Could not extract date, using default scroll count\")\n        return 50\n    \n    print(f\"Earliest date after 3 scrolls: {earliest_3_scroll.strftime('%Y-%m-%d')}\")\n    \n    time_diff = earliest_1_scroll - earliest_3_scroll\n    days_per_2_scrolls = time_diff.days\n    \n    if days_per_2_scrolls <= 0:\n        print(\"Time difference too small, using default scroll count\")\n        return 50\n    \n    print(f\"Time covered by 2 scrolls: {days_per_2_scrolls} days\")\n    \n    days_to_cutoff = (earliest_3_scroll - CUTOFF_DATE).days\n    \n    if days_to_cutoff <= 0:\n        print(f\"Already reached cutoff date!\")\n        return 3\n    \n    additional_scrolls = int((days_to_cutoff / days_per_2_scrolls) * 2) + 5\n    total_scrolls = 3 + additional_scrolls\n    \n    print(f\"Days to cutoff: {days_to_cutoff}\")\n    print(f\"Estimated total scrolls needed: {total_scrolls}\")\n    \n    if total_scrolls > 200:\n        print(\"Capping at 200 scrolls\")\n        return 200\n    \n    return total_scrolls"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract post stats\ndef extract_post_stats():\n    likes = \"0\"\n    comments = \"0\"\n    is_paid = False\n    \n    try:\n        like_spans = driver.find_elements(By.CSS_SELECTOR, 'span.x1ypdohk.x1s688f.x2fvf9.xe9ewy2[role=\"button\"]')\n        if like_spans:\n            likes = like_spans[0].text\n    except:\n        pass\n    \n    try:\n        comment_spans = driver.find_elements(\n            By.CSS_SELECTOR,\n            'span.xdj266r.x14z9mp.xat24cr.x1lziwak.xexx8yu.xyri2b.x18d9i69.x1c1uobl.x1hl2dhg.x16tdsg8.x1vvkbs'\n        )\n        for span in comment_spans:\n            text = span.text\n            if text.replace(',', '').isdigit():\n                comments = text\n                break\n    except:\n        pass\n    \n    try:\n        if \"Paid partnership with \" in driver.page_source:\n            is_paid = True\n    except:\n        pass\n    \n    return likes, comments, is_paid\n\n# Extract posting account (subbrand)\ndef extract_posting_account():\n    try:\n        # Look for the posting account span (e.g., nikerunning as subbrand of nike)\n        account_spans = driver.find_elements(\n            By.CSS_SELECTOR,\n            'span.x193iq5w.xeuugli.x1fj9vlw.x13faqbe.x1vvkbs.xt0psk2.x1i0vuye.xvs91rp.x1s688f.x5n08af.x10wh9bi.xpm28yp.x8viiok.x1o7cslx'\n        )\n        if account_spans:\n            # Return the first one that looks like a username\n            for span in account_spans:\n                text = span.text.strip()\n                if text and len(text) > 0 and not text.isdigit():\n                    return text\n    except:\n        pass\n    return None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract current carousel item media URLs (returns ALL valid images/videos)\ndef extract_current_item_urls():\n    media_urls = []\n    \n    try:\n        # Check for video first (priority)\n        videos = driver.find_elements(By.TAG_NAME, 'video')\n        for video in videos:\n            src = video.get_attribute('src')\n            if src and ('cdninstagram.com' in src or 'fbcdn.net' in src):\n                if src not in seen_urls:\n                    media_urls.append((src, 'video'))\n        \n        # Get ALL valid images (not just first one)\n        images = driver.find_elements(By.TAG_NAME, 'img')\n        \n        for img in images:\n            src = img.get_attribute('src')\n            if src and ('cdninstagram.com' in src or 'fbcdn.net' in src):\n                # Filter out unwanted images\n                if any(x in src for x in ['/s150x150/', '/s320x320/', 's640x640', 'static']):\n                    continue\n                \n                # Filter out 150x150 profile pics by dimension\n                try:\n                    width = img.get_attribute('width')\n                    height = img.get_attribute('height')\n                    if width and height:\n                        if int(width) == 150 and int(height) == 150:\n                            continue\n                except:\n                    pass\n                \n                # Skip if we've already seen this URL\n                if src in seen_urls:\n                    continue\n                \n                # Add ALL valid images (better to have more than miss content)\n                media_urls.append((src, 'image'))\n        \n    except Exception as e:\n        print(f\"  Error extracting media: {e}\")\n    \n    return media_urls"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if next button exists\n",
    "def has_next_button():\n",
    "    try:\n",
    "        selectors = [\n",
    "            'button[aria-label=\"Next\"]',\n",
    "            'button[aria-label=\"next\"]',\n",
    "            'button._afxw._al46._al47'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                next_btn = WebDriverWait(driver, 2).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, selector))\n",
    "                )\n",
    "                return next_btn\n",
    "            except:\n",
    "                continue\n",
    "        return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download single file\n",
    "def download_file(url, filepath):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check for duplicates\n",
    "        content_hash = hashlib.md5(response.content).hexdigest()\n",
    "        if content_hash in downloaded_hashes:\n",
    "            return False, \"duplicate\"\n",
    "        \n",
    "        downloaded_hashes.add(content_hash)\n",
    "        filepath.write_bytes(response.content)\n",
    "        return True, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, str(e)[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Scrape a single post (item-by-item download, all images per position)\ndef scrape_post(post_url):\n    print(f\"\\n{'='*60}\")\n    print(f\"Scraping: {post_url}\")\n    print('='*60)\n    \n    driver.get(post_url)\n    time.sleep(6 * SLEEP_MULTIPLIER)\n    \n    # Extract date\n    post_date = extract_post_date()\n    if post_date:\n        print(f\"Date: {post_date.strftime('%Y-%m-%d %H:%M:%S')}\")\n    \n    # Extract stats\n    likes, comments, is_paid = extract_post_stats()\n    \n    # Extract posting account (subbrand)\n    posting_account = extract_posting_account()\n    if posting_account:\n        print(f\"Posted by: {posting_account}\")\n    \n    print(f\"Stats: Likes={likes}, Comments={comments}, Paid={is_paid}\")\n    \n    # Extract caption\n    caption = extract_post_caption()\n    \n    # Create directory for this post\n    post_id = post_url.rstrip('/').split('/')[-1]\n    post_dir = DOWNLOAD_DIR / post_id\n    post_dir.mkdir(exist_ok=True)\n    \n    # Save caption\n    if caption:\n        caption_file = post_dir / \"caption.txt\"\n        caption_file.write_text(caption, encoding='utf-8')\n        print(f\"Saved caption ({len(caption)} chars)\")\n    \n    # Download carousel items one by one\n    carousel_position = 1\n    total_items_downloaded = 0\n    \n    while True:\n        # Extract ALL media URLs at current position\n        media_urls = extract_current_item_urls()\n        \n        if media_urls:\n            print(f\"\\n  Carousel position {carousel_position}: Found {len(media_urls)} items\")\n            \n            # Download each media item\n            for url, media_type in media_urls:\n                ext = '.mp4' if media_type == 'video' else '.jpg'\n                \n                # Find next available filename\n                item_num = 1\n                while True:\n                    filepath = post_dir / f\"item_{item_num}{ext}\"\n                    if not filepath.exists():\n                        break\n                    item_num += 1\n                \n                success, error = download_file(url, filepath)\n                if success:\n                    print(f\"    Downloaded item_{item_num}{ext} ({media_type})\")\n                    seen_urls.add(url)  # Mark URL as seen\n                    total_items_downloaded += 1\n                else:\n                    if error == \"duplicate\":\n                        print(f\"    Skipped item_{item_num}: Duplicate content\")\n                        seen_urls.add(url)  # Still mark as seen\n                    else:\n                        print(f\"    Failed item_{item_num}: {error}\")\n                \n                time.sleep(5 * SLEEP_MULTIPLIER)\n        \n        # Try to click next\n        next_btn = has_next_button()\n        if not next_btn:\n            break\n        \n        next_btn.click()\n        time.sleep(1 * SLEEP_MULTIPLIER)\n        carousel_position += 1\n    \n    print(f\"\\nProcessed {carousel_position} carousel positions\")\n    \n    # Log stats\n    stats_log.append({\n        'timestamp': datetime.now().isoformat(),\n        'post_url': post_url,\n        'post_id': post_id,\n        'post_date': post_date.isoformat() if post_date else None,\n        'posting_account': posting_account,\n        'likes': likes,\n        'comments': comments,\n        'paid_partnership': is_paid,\n        'caption_length': len(caption) if caption else 0,\n        'carousel_positions': carousel_position,\n        'media_downloaded': total_items_downloaded\n    })\n    \n    print(f\"✓ Downloaded {total_items_downloaded} unique items to '{post_dir}'\")\n    return total_items_downloaded"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Scrape user profile\ndef scrape_user(username, max_posts=None):\n    print(f\"\\n{'='*60}\")\n    print(f\"Scraping user: {username}\")\n    print(f\"Cutoff date: {CUTOFF_DATE.strftime('%Y-%m-%d')}\")\n    print('='*60)\n    \n    # Estimate scrolls needed\n    estimated_scrolls = estimate_scrolls_needed(username)\n    \n    # Scroll profile and collect links incrementally\n    print(f\"\\nScrolling {estimated_scrolls} times...\")\n    profile_url = f\"https://www.instagram.com/{username}/\"\n    driver.get(profile_url)\n    time.sleep(6 * SLEEP_MULTIPLIER)\n    \n    all_post_links = set()\n    \n    for i in range(estimated_scrolls):\n        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n        time.sleep(4 * SLEEP_MULTIPLIER)\n        \n        # Collect links after each scroll\n        links = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"/p/\"], a[href*=\"/reel/\"]')\n        for link in links:\n            href = link.get_attribute('href')\n            if href and ('/p/' in href or '/reel/' in href):\n                all_post_links.add(href)\n        \n        if (i + 1) % 10 == 0:\n            print(f\"  Scrolled {i + 1}/{estimated_scrolls} times... ({len(all_post_links)} posts collected so far)\")\n    \n    print(f\"Completed {estimated_scrolls} scrolls\")\n    \n    # Convert set to list\n    post_links = list(all_post_links)\n    \n    if max_posts:\n        post_links = post_links[:max_posts]\n    \n    print(f\"Found {len(post_links)} unique posts (including reels)\")\n    \n    # Scrape each post\n    try:\n        total_downloaded = 0\n        skipped_old = 0\n        \n        for i, post_url in enumerate(post_links, 1):\n            print(f\"\\nPost {i}/{len(post_links)}\")\n            \n            # Check post date before downloading\n            driver.get(post_url)\n            time.sleep(3 * SLEEP_MULTIPLIER)\n            \n            post_date = extract_post_date()\n            if post_date and post_date < CUTOFF_DATE:\n                print(f\"⏩ Skipping post from {post_date.strftime('%Y-%m-%d')} (before cutoff)\")\n                skipped_old += 1\n                continue\n            \n            # Go back to post URL to scrape it properly\n            count = scrape_post(post_url)\n            total_downloaded += count\n            time.sleep(4 * SLEEP_MULTIPLIER)\n        \n        print(f\"\\n✓ User '{username}' complete:\")\n        print(f\"  Items downloaded: {total_downloaded}\")\n        print(f\"  Posts skipped (before cutoff): {skipped_old}\")\n        return total_downloaded\n        \n    except Exception as e:\n        print(f\"Error scraping user: {e}\")\n        return 0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main scraping loop\n",
    "if USERS_TO_SCRAPE:\n",
    "    for username in USERS_TO_SCRAPE:\n",
    "        scrape_user(username, max_posts=None)\n",
    "        time.sleep(6 * SLEEP_MULTIPLIER)\n",
    "else:\n",
    "    print(\"⚠ No users to scrape. Add usernames to USERS_TO_SCRAPE list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stats log\n",
    "if stats_log:\n",
    "    stats_file = DOWNLOAD_DIR / \"scrape_stats.json\"\n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(stats_log, f, indent=2)\n",
    "    print(f\"\\n✓ Stats saved to {stats_file}\")\n",
    "    print(f\"\\nTotal stats:\")\n",
    "    print(f\"  Posts scraped: {len(stats_log)}\")\n",
    "    print(f\"  Items downloaded: {sum(s['media_downloaded'] for s in stats_log)}\")\n",
    "    print(f\"  Paid partnerships: {sum(1 for s in stats_log if s['paid_partnership'])}\")\n",
    "else:\n",
    "    print(\"No stats to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close browser\n",
    "driver.quit()\n",
    "print(\"\\n✓ Browser closed\")\n",
    "print(\"✓ All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}