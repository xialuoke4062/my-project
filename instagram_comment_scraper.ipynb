{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Instagram Comment Scraper (DOM-based, Mobile View)\n",
    "Downloads comments for existing post folders\n",
    "Incremental downloading with scroll-based collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USERNAME = \"\"  # Your Instagram username\n",
    "PASSWORD = \"\"  # Your Instagram password\n",
    "\n",
    "# List of usernames whose posts to scrape comments from\n",
    "USERS_TO_SCRAPE = [\"nike\"]  # e.g., [\"user1\", \"user2\", \"user3\"]\n",
    "\n",
    "# Sleep multiplier - set to 2 or 3 to slow down (default 1)\n",
    "SLEEP_MULTIPLIER = 2  # Change to 2 or 3 if you need slower execution\n",
    "\n",
    "BASE_DOWNLOAD_DIR = Path(\"instagram_downloads\")\n",
    "\n",
    "# Logging setup\n",
    "log_lines = []\n",
    "original_print = print\n",
    "\n",
    "def custom_print(*args, **kwargs):\n",
    "    # Capture the message\n",
    "    message = ' '.join(str(arg) for arg in args)\n",
    "    log_lines.append(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
    "    # Call original print\n",
    "    original_print(*args, **kwargs)\n",
    "\n",
    "print = custom_print\n",
    "\n",
    "# Current user being scraped\n",
    "CURRENT_USER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Chrome with mobile emulation\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('mobileEmulation', {\n",
    "    'deviceName': 'iPhone 12 Pro'\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "print(\"‚úì Browser opened with mobile emulation\")\n",
    "print(f\"Sleep multiplier: {SLEEP_MULTIPLIER}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Instagram\n",
    "def login_instagram(username, password):\n",
    "    driver.get('https://www.instagram.com/')\n",
    "    time.sleep(6 * SLEEP_MULTIPLIER)\n",
    "    \n",
    "    try:\n",
    "        # Wait for and click login button if on homepage\n",
    "        try:\n",
    "            login_link = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//a[contains(@href, '/accounts/login')]\"))\n",
    "            )\n",
    "            login_link.click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Enter username\n",
    "        username_input = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"username\"))\n",
    "        )\n",
    "        username_input.send_keys(username)\n",
    "        \n",
    "        # Enter password\n",
    "        password_input = driver.find_element(By.NAME, \"password\")\n",
    "        password_input.send_keys(password)\n",
    "        password_input.send_keys(Keys.RETURN)\n",
    "                \n",
    "        # Handle \"Save Your Login Info\" popup\n",
    "        try:\n",
    "            not_now = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not now') or contains(text(), 'Not Now')]\"))\n",
    "            )\n",
    "            not_now.click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Handle \"Turn on Notifications\" popup\n",
    "        try:\n",
    "            not_now = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not Now')]\"))\n",
    "            )\n",
    "            not_now.click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(\"‚úì Logged in successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Perform login\n",
    "if USERNAME and PASSWORD:\n",
    "    login_instagram(USERNAME, PASSWORD)\n",
    "else:\n",
    "    print(\"‚ö† No login credentials provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract comments from current page\n",
    "def extract_comments():\n",
    "    comments = []\n",
    "    \n",
    "    try:\n",
    "        # Find all comment sections\n",
    "        comment_sections = driver.find_elements(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.html-div.xdj266r.x14z9mp.xat24cr.x1lziwak.xyri2b.x1c1uobl.x9f619.xjbqb8w.x78zum5.x15mokao.x1ga7v0g.x16uus16.xbiv7yw.xsag5q8.xz9dl7a.x1uhb9sk.x1plvlek.xryxfnj.x1c4vz4f.x2lah0s.x1q0g3np.xqjyukv.x1qjc9v5.x1oa3qoh.x1nhvcw1'\n",
    "        )\n",
    "        \n",
    "        for section in comment_sections:\n",
    "            try:\n",
    "                comment_data = {}\n",
    "                \n",
    "                # Extract author (looking for the _aacw class which indicates username)\n",
    "                author_spans = section.find_elements(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    'span._ap3a._aaco._aacw._aacx._aad7._aade'\n",
    "                )\n",
    "                if author_spans:\n",
    "                    comment_data['author'] = author_spans[0].text\n",
    "                else:\n",
    "                    continue  # Skip if no author found\n",
    "                \n",
    "                # Extract comment text (looking for _aacu class which indicates regular text)\n",
    "                text_spans = section.find_elements(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    'span._ap3a._aaco._aacu._aacx._aad7._aade'\n",
    "                )\n",
    "                \n",
    "                # Filter out the author span from text spans\n",
    "                comment_text = \"\"\n",
    "                for span in text_spans:\n",
    "                    text = span.text.strip()\n",
    "                    if text and text != comment_data['author']:\n",
    "                        comment_text += text + \" \"\n",
    "                \n",
    "                comment_data['text'] = comment_text.strip()\n",
    "                \n",
    "                # Check if comment has media (image/gif)\n",
    "                images = section.find_elements(By.TAG_NAME, 'img')\n",
    "                has_media = False\n",
    "                for img in images:\n",
    "                    src = img.get_attribute('src')\n",
    "                    # Filter out profile pictures and emojis\n",
    "                    if src and 'cdninstagram.com' in src and 's150x150' not in src:\n",
    "                        has_media = True\n",
    "                        comment_data['media_url'] = src\n",
    "                        break\n",
    "                \n",
    "                if has_media and not comment_data['text']:\n",
    "                    comment_data['text'] = \"[Image/GIF]\"\n",
    "                \n",
    "                # Extract timestamp\n",
    "                time_elements = section.find_elements(By.TAG_NAME, 'time')\n",
    "                if time_elements:\n",
    "                    datetime_str = time_elements[0].get_attribute('datetime')\n",
    "                    title_str = time_elements[0].get_attribute('title')\n",
    "                    comment_data['timestamp'] = datetime_str\n",
    "                    comment_data['timestamp_display'] = title_str\n",
    "                \n",
    "                # Extract likes if present\n",
    "                like_spans = section.find_elements(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    'span.x1lliihq.x1plvlek.xryxfnj.x1n2onr6.xyejjpt.x15dsfln.x193iq5w.xeuugli.x1fj9vlw.x13faqbe.x1vvkbs.x1s928wv.xhkezso.x1gmr53x.x1cpjm7i.x1fgarty.x1943h6x.x1i0vuye.x1fhwpqd.x1s688f.x1roi4f4.x1s3etm8.x676frb.x10wh9bi.xpm28yp.x8viiok.x1o7cslx'\n",
    "                )\n",
    "                for span in like_spans:\n",
    "                    text = span.text.strip()\n",
    "                    if 'like' in text.lower():\n",
    "                        comment_data['likes'] = text\n",
    "                        break\n",
    "                \n",
    "                # Only add if we have both author and text/media\n",
    "                if comment_data.get('author') and (comment_data.get('text') or comment_data.get('media_url')):\n",
    "                    comments.append(comment_data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Skip individual comment if extraction fails\n",
    "                continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error extracting comments: {e}\")\n",
    "    \n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape comments for a single post\n",
    "def scrape_post_comments(post_id, post_dir):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping comments for post: {post_id}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Navigate to comments page\n",
    "    comments_url = f\"https://www.instagram.com/p/{post_id}/comments/\"\n",
    "    driver.get(comments_url)\n",
    "    time.sleep(6 * SLEEP_MULTIPLIER)\n",
    "    \n",
    "    all_comments = []\n",
    "    seen_comment_ids = set()\n",
    "    \n",
    "    # Read expected comment count from scrape_stats.json if available\n",
    "    expected_comments = None\n",
    "    stats_file = post_dir.parent / \"scrape_stats.json\"\n",
    "    if stats_file.exists():\n",
    "        try:\n",
    "            with open(stats_file, 'r') as f:\n",
    "                stats = json.load(f)\n",
    "                for stat in stats:\n",
    "                    if stat['post_id'] == post_id:\n",
    "                        comments_str = stat.get('comments', '0')\n",
    "                        # Parse comment count (handle formats like \"1,234\")\n",
    "                        expected_comments = int(comments_str.replace(',', ''))\n",
    "                        print(f\"Expected comments: {expected_comments}\")\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read expected comments: {e}\")\n",
    "    \n",
    "    # Scroll and collect comments incrementally\n",
    "    scroll_count = 0\n",
    "    no_new_comments_count = 0\n",
    "    max_scrolls = 200  # Safety limit\n",
    "    \n",
    "    print(\"Starting to scroll and collect comments...\")\n",
    "    \n",
    "    while scroll_count < max_scrolls:\n",
    "        # Extract comments from current view\n",
    "        comments = extract_comments()\n",
    "        \n",
    "        # Add new comments to collection\n",
    "        new_comments = 0\n",
    "        for comment in comments:\n",
    "            # Create unique ID from author + timestamp\n",
    "            comment_id = f\"{comment.get('author', '')}_{comment.get('timestamp', '')}\"\n",
    "            if comment_id not in seen_comment_ids:\n",
    "                all_comments.append(comment)\n",
    "                seen_comment_ids.add(comment_id)\n",
    "                new_comments += 1\n",
    "        \n",
    "        print(f\"  Scroll {scroll_count + 1}: Found {new_comments} new comments (Total: {len(all_comments)})\")\n",
    "        \n",
    "        # Save comments incrementally every 10 scrolls\n",
    "        if (scroll_count + 1) % 10 == 0:\n",
    "            comments_file = post_dir / \"comments.json\"\n",
    "            with open(comments_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_comments, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"  üíæ Saved {len(all_comments)} comments to {comments_file}\")\n",
    "        \n",
    "        # Check if we should stop\n",
    "        if new_comments == 0:\n",
    "            no_new_comments_count += 1\n",
    "            if no_new_comments_count >= 3:\n",
    "                print(\"  No new comments found after 3 scrolls, stopping\")\n",
    "                break\n",
    "        else:\n",
    "            no_new_comments_count = 0\n",
    "        \n",
    "        # Check if we've reached expected count (with some tolerance)\n",
    "        if expected_comments and len(all_comments) >= expected_comments * 0.9:\n",
    "            print(f\"  Reached ~{int((len(all_comments) / expected_comments) * 100)}% of expected comments\")\n",
    "            # Continue for a few more scrolls to be sure\n",
    "            if len(all_comments) >= expected_comments:\n",
    "                break\n",
    "        \n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3 * SLEEP_MULTIPLIER)\n",
    "        scroll_count += 1\n",
    "    \n",
    "    # Final save\n",
    "    comments_file = post_dir / \"comments.json\"\n",
    "    with open(comments_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_comments, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n‚úì Scraped {len(all_comments)} comments\")\n",
    "    if expected_comments:\n",
    "        percentage = int((len(all_comments) / expected_comments) * 100)\n",
    "        print(f\"  Coverage: {percentage}% of expected {expected_comments} comments\")\n",
    "    print(f\"‚úì Saved to {comments_file}\")\n",
    "    \n",
    "    return len(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape comments for all posts of a user\n",
    "def scrape_user_comments(username):\n",
    "    global CURRENT_USER, log_lines\n",
    "    \n",
    "    CURRENT_USER = username\n",
    "    user_dir = BASE_DOWNLOAD_DIR / username\n",
    "    \n",
    "    if not user_dir.exists():\n",
    "        print(f\"‚ö† User directory not found: {user_dir}\")\n",
    "        print(\"  Please run the post scraper first to create post folders\")\n",
    "        return\n",
    "    \n",
    "    # Reset log for this user\n",
    "    log_lines = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping comments for user: {username}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Find all post directories\n",
    "    post_dirs = [d for d in user_dir.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(post_dirs)} post folders\")\n",
    "    \n",
    "    total_comments = 0\n",
    "    \n",
    "    for i, post_dir in enumerate(post_dirs, 1):\n",
    "        post_id = post_dir.name\n",
    "        print(f\"\\nPost {i}/{len(post_dirs)}: {post_id}\")\n",
    "        \n",
    "        # Skip if comments already exist (optional - comment out to re-scrape)\n",
    "        comments_file = post_dir / \"comments.json\"\n",
    "        if comments_file.exists():\n",
    "            print(f\"  ‚è© Comments already exist, skipping\")\n",
    "            try:\n",
    "                with open(comments_file, 'r') as f:\n",
    "                    existing = json.load(f)\n",
    "                    total_comments += len(existing)\n",
    "                    print(f\"  Found {len(existing)} existing comments\")\n",
    "            except:\n",
    "                pass\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            count = scrape_post_comments(post_id, post_dir)\n",
    "            total_comments += count\n",
    "            time.sleep(5 * SLEEP_MULTIPLIER)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error scraping comments: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úì User '{username}' complete\")\n",
    "    print(f\"  Total comments scraped: {total_comments}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Save log file\n",
    "    if log_lines:\n",
    "        log_file = user_dir / \"comments_log.txt\"\n",
    "        with open(log_file, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(log_lines))\n",
    "        print(f\"‚úì Log saved to {log_file}\")\n",
    "    \n",
    "    return total_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main scraping loop\n",
    "if USERS_TO_SCRAPE:\n",
    "    for username in USERS_TO_SCRAPE:\n",
    "        scrape_user_comments(username)\n",
    "        time.sleep(6 * SLEEP_MULTIPLIER)\n",
    "else:\n",
    "    print(\"‚ö† No users to scrape. Add usernames to USERS_TO_SCRAPE list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close browser\n",
    "driver.quit()\n",
    "print(\"\\n‚úì Browser closed\")\n",
    "print(\"‚úì All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
