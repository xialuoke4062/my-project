{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram Multi-User Scraper (DOM-based)\n",
    "Uses BeautifulSoup to extract media URLs directly from DOM - no messy network logs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USERNAME = \"\"  # Your Instagram username\n",
    "PASSWORD = \"\"  # Your Instagram password\n",
    "\n",
    "# List of usernames to scrape\n",
    "USERS_TO_SCRAPE = []  # e.g., [\"user1\", \"user2\", \"user3\"]\n",
    "\n",
    "DOWNLOAD_DIR = Path(\"instagram_downloads\")\n",
    "DOWNLOAD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Global tracking\n",
    "downloaded_hashes = set()\n",
    "stats_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Chrome (desktop mode for better stability)\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--start-maximized')\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "print(\"✓ Browser opened\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Instagram\n",
    "def login_instagram(username, password):\n",
    "    driver.get('https://www.instagram.com/')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        # Enter username\n",
    "        username_input = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"username\"))\n",
    "        )\n",
    "        username_input.send_keys(username)\n",
    "        \n",
    "        # Enter password\n",
    "        password_input = driver.find_element(By.NAME, \"password\")\n",
    "        password_input.send_keys(password)\n",
    "        password_input.send_keys(Keys.RETURN)\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Handle \"Save Your Login Info\" popup\n",
    "        try:\n",
    "            not_now = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not now') or contains(text(), 'Not Now')]\"))\n",
    "            )\n",
    "            not_now.click()\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Handle \"Turn on Notifications\" popup\n",
    "        try:\n",
    "            not_now = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not Now')]\"))\n",
    "            )\n",
    "            not_now.click()\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(\"✓ Logged in successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Perform login\n",
    "if USERNAME and PASSWORD:\n",
    "    login_instagram(USERNAME, PASSWORD)\n",
    "else:\n",
    "    print(\"⚠ No credentials - add USERNAME and PASSWORD above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract post stats\n",
    "def extract_post_stats():\n",
    "    likes = \"0\"\n",
    "    comments = \"0\"\n",
    "    is_paid = False\n",
    "    \n",
    "    try:\n",
    "        # Get likes - look for button with \"like\" text\n",
    "        like_buttons = driver.find_elements(By.XPATH, \"//button[contains(@class, 'x1i10hfl')]//span[contains(text(), 'like')]\")\n",
    "        if like_buttons:\n",
    "            like_text = like_buttons[0].text\n",
    "            # Extract number from text like \"123 likes\"\n",
    "            likes = like_text.split()[0] if like_text else \"0\"\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Get comments - look for elements with comment counts\n",
    "        comment_elements = driver.find_elements(By.XPATH, \"//span[contains(text(), 'comment')]\")\n",
    "        for elem in comment_elements:\n",
    "            text = elem.text\n",
    "            if 'comment' in text.lower():\n",
    "                parts = text.split()\n",
    "                if parts and parts[0].replace(',', '').isdigit():\n",
    "                    comments = parts[0]\n",
    "                    break\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Check for paid partnership\n",
    "        if \"Paid partnership with \" in driver.page_source:\n",
    "            is_paid = True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return likes, comments, is_paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract media URL from current view using BeautifulSoup\n",
    "def extract_media_url():\n",
    "    try:\n",
    "        # Find the main article/post container\n",
    "        article = driver.find_element(By.TAG_NAME, 'article')\n",
    "        html = article.get_attribute('innerHTML')\n",
    "        soup = bs(html, 'html.parser')\n",
    "        \n",
    "        # Try to find video first, then image\n",
    "        video = soup.find('video')\n",
    "        if video and video.get('src'):\n",
    "            return video['src']\n",
    "        \n",
    "        # Look for img tag\n",
    "        img = soup.find('img', src=True)\n",
    "        if img and img.get('src'):\n",
    "            src = img['src']\n",
    "            # Filter out profile pics and small icons\n",
    "            if 'cdninstagram.com' in src or 'fbcdn.net' in src:\n",
    "                if '/s150x150/' not in src and '/s320x320/' not in src:\n",
    "                    return src\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error extracting media: {e}\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download media file with duplicate detection\n",
    "def download_media(url, filepath):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check for duplicates\n",
    "        content_hash = hashlib.md5(response.content).hexdigest()\n",
    "        if content_hash in downloaded_hashes:\n",
    "            return False, \"duplicate\"\n",
    "        \n",
    "        downloaded_hashes.add(content_hash)\n",
    "        \n",
    "        # Write file\n",
    "        filepath.write_bytes(response.content)\n",
    "        return True, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, str(e)[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if carousel has next button\n",
    "def has_next_in_carousel():\n",
    "    try:\n",
    "        # Try multiple selectors for next button\n",
    "        selectors = [\n",
    "            'button[aria-label=\"Next\"]',\n",
    "            'button[aria-label=\"next\"]',\n",
    "            'button._afxw._al46._al47',\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                next_btn = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                if next_btn.is_displayed():\n",
    "                    return next_btn\n",
    "            except:\n",
    "                continue\n",
    "        return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all media from current post (handles single image, video, or carousel)\n",
    "def scrape_current_post(post_dir, post_num):\n",
    "    media_count = 0\n",
    "    \n",
    "    # Extract first media\n",
    "    url = extract_media_url()\n",
    "    if url:\n",
    "        media_count += 1\n",
    "        ext = '.mp4' if 'video' in url or '.mp4' in url else '.jpg'\n",
    "        filepath = post_dir / f\"post{post_num}_item{media_count}{ext}\"\n",
    "        \n",
    "        success, error = download_media(url, filepath)\n",
    "        if success:\n",
    "            print(f\"    Downloaded: {filepath.name}\")\n",
    "        else:\n",
    "            if error == \"duplicate\":\n",
    "                print(f\"    Skipped: {filepath.name} (duplicate)\")\n",
    "            else:\n",
    "                print(f\"    Failed: {error}\")\n",
    "    \n",
    "    # Check for carousel and click through\n",
    "    while True:\n",
    "        next_btn = has_next_in_carousel()\n",
    "        if not next_btn:\n",
    "            break\n",
    "        \n",
    "        next_btn.click()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Extract next media\n",
    "        url = extract_media_url()\n",
    "        if url:\n",
    "            media_count += 1\n",
    "            ext = '.mp4' if 'video' in url or '.mp4' in url else '.jpg'\n",
    "            filepath = post_dir / f\"post{post_num}_item{media_count}{ext}\"\n",
    "            \n",
    "            success, error = download_media(url, filepath)\n",
    "            if success:\n",
    "                print(f\"    Downloaded: {filepath.name}\")\n",
    "            else:\n",
    "                if error == \"duplicate\":\n",
    "                    print(f\"    Skipped: {filepath.name} (duplicate)\")\n",
    "                else:\n",
    "                    print(f\"    Failed: {error}\")\n",
    "    \n",
    "    return media_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to next post in modal view\n",
    "def go_to_next_post():\n",
    "    try:\n",
    "        # Find next post button (right arrow in modal)\n",
    "        selectors = [\n",
    "            'a[role=\"button\"][aria-label=\"Next\"]',\n",
    "            'a[aria-label=\"Next\"]',\n",
    "            'button.coreSpriteRightPaginationArrow',\n",
    "            'a._aaqg._aaqh'  # Common next post button class\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                next_post_btn = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                if next_post_btn.is_displayed():\n",
    "                    next_post_btn.click()\n",
    "                    time.sleep(2)\n",
    "                    return True\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return False\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all posts from a user\n",
    "def scrape_user(username, max_posts=None):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping user: {username}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Navigate to user profile\n",
    "    profile_url = f\"https://www.instagram.com/{username}/\"\n",
    "    driver.get(profile_url)\n",
    "    time.sleep(4)\n",
    "    \n",
    "    # Create user directory\n",
    "    user_dir = DOWNLOAD_DIR / username\n",
    "    user_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Click on first post\n",
    "        first_post = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, 'article a[href*=\"/p/\"]'))\n",
    "        )\n",
    "        first_post.click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "        post_count = 0\n",
    "        total_media = 0\n",
    "        \n",
    "        while True:\n",
    "            if max_posts and post_count >= max_posts:\n",
    "                print(f\"\\nReached max posts limit ({max_posts})\")\n",
    "                break\n",
    "            \n",
    "            post_count += 1\n",
    "            print(f\"\\nPost {post_count}:\")\n",
    "            \n",
    "            # Extract stats\n",
    "            likes, comments, is_paid = extract_post_stats()\n",
    "            print(f\"  Stats: Likes={likes}, Comments={comments}, Paid={is_paid}\")\n",
    "            \n",
    "            # Download all media from this post\n",
    "            media_count = scrape_current_post(user_dir, post_count)\n",
    "            total_media += media_count\n",
    "            \n",
    "            # Get current post URL\n",
    "            current_url = driver.current_url\n",
    "            post_id = current_url.rstrip('/').split('/')[-1] if '/p/' in current_url else f\"post{post_count}\"\n",
    "            \n",
    "            # Log stats\n",
    "            stats_log.append({\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'username': username,\n",
    "                'post_url': current_url,\n",
    "                'post_id': post_id,\n",
    "                'likes': likes,\n",
    "                'comments': comments,\n",
    "                'paid_partnership': is_paid,\n",
    "                'media_downloaded': media_count\n",
    "            })\n",
    "            \n",
    "            # Go to next post\n",
    "            if not go_to_next_post():\n",
    "                print(\"\\nNo more posts found\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n✓ User '{username}' complete: {post_count} posts, {total_media} total media\")\n",
    "        return post_count, total_media\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping user: {e}\")\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main scraping loop - scrape all users\n",
    "if USERS_TO_SCRAPE:\n",
    "    for username in USERS_TO_SCRAPE:\n",
    "        scrape_user(username, max_posts=None)  # Set max_posts if needed\n",
    "        time.sleep(3)\n",
    "else:\n",
    "    print(\"⚠ No users to scrape. Add usernames to USERS_TO_SCRAPE list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stats log\n",
    "if stats_log:\n",
    "    stats_file = DOWNLOAD_DIR / \"scrape_stats.json\"\n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(stats_log, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ Stats saved to {stats_file}\")\n",
    "    print(f\"\\nTotal Summary:\")\n",
    "    print(f\"  Posts scraped: {len(stats_log)}\")\n",
    "    print(f\"  Items downloaded: {sum(s['media_downloaded'] for s in stats_log)}\")\n",
    "    print(f\"  Paid partnerships: {sum(1 for s in stats_log if s['paid_partnership'])}\")\n",
    "    print('='*60)\n",
    "else:\n",
    "    print(\"No stats to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close browser\n",
    "driver.quit()\n",
    "print(\"\\n✓ Browser closed\")\n",
    "print(\"✓ All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
